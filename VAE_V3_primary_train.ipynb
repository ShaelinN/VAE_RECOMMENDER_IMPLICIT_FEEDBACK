{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_V3_primary_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GZ8A_Q0_gVfR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaelinN/VAE_RECOMMENDER_IMPLICIT_FEEDBACK/blob/main/VAE_V3_primary_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFjVqXpgQ-E"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rNgvglTWCN"
      },
      "source": [
        "#!pip3 install tensorflow-ranking\n",
        "#!pip3 install tensorflow\n",
        "#!pip3 install numpy\n",
        "#!pip3 install pickle\n",
        "#!pip3 install math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as tfback\n",
        "from tensorflow.keras import layers, activations, Model, losses\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "#import tensorflow_ranking as tfr\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pRl4vBS7LM"
      },
      "source": [
        "\"\"\"\n",
        "#uncomment if running as .py on cluster such as HIPPO\n",
        "#comment if running as .ipynb on Google Colab\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input_data', type=str)\n",
        "parser.add_argument('--training_results', type=str, default=\"training_results\")\n",
        "\n",
        "parser.add_argument('--prim_pel', type=str, default=\"prim_pel.txt\")\n",
        "parser.add_argument('--prim_pbl', type=str, default=\"prim_pbl.txt\")\n",
        "parser.add_argument('--prim_pevl', type=str, default=\"prim_pevl.txt\")\n",
        "\n",
        "parser.add_argument('--sec_pel', type=str, default=\"sec_pel.txt\")\n",
        "parser.add_argument('--sec_pbl', type=str, default=\"sec_pbl.txt\")\n",
        "parser.add_argument('--sec_pevl', type=str, default=\"sec_pevl.txt\")\n",
        "\n",
        "\n",
        "parser.add_argument('--weights', type=str)\n",
        "parser.add_argument('--intermediatedim', type=int, default=512)\n",
        "parser.add_argument('--epochs', type=int, default=5)\n",
        "parser.add_argument('--batchsize', type=int, default=200)\n",
        "parser.add_argument('--klannealrate', type=float, default=0.0001)\n",
        "args = parser.parse_args()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akM7uK8m5Jw3"
      },
      "source": [
        "#uncomment if running as .ipynb on Google Colabas\n",
        "#comment if running as  .py on cluster such as HIPPO\n",
        "class argclass(object):\n",
        "  def __init__(self):\n",
        "    self.root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "    self.input_data = \"Data/remove_low_interaction_users/split/matrices/implicit\"\n",
        "    self.training_results =\"training_results\"\n",
        "\n",
        "    self.prim_pel = \"prim_pel.txt\"\n",
        "    self.prim_pbl = \"prim_pbl.txt\"\n",
        "    self.prim_pevl = \"prim_pevl.txt\"\n",
        "\n",
        "    self.sec_pel = \"sec_pel.txt\"\n",
        "    self.sec_pbl = \"sec_pbl.txt\"\n",
        "    self.sec_pevl = \"sec_pevl.txt\"\n",
        "\n",
        "    self.weights = None #\"vae_epoch_01_loss_210.97.hdf5\"\n",
        "    self.intermediatedim = 512 \n",
        "    self.epochs = 5\n",
        "    self.batchsize = 200\n",
        "    self.klannealrate = 0.0001\n",
        "args = argclass()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aj-nLb6kRj5",
        "outputId": "5cb0b2b7-45ad-436a-a6ba-a83bbce887af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtzkG8xR_zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f2f0d9-e1e1-471b-9db3-336db3546126"
      },
      "source": [
        "root = args.root\n",
        "\n",
        "input_data = args.input_data\n",
        "input_data = os.path.join(root, input_data)\n",
        "\n",
        "training_results = args.training_results\n",
        "training_results = os.path.join(root, training_results)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_results)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "#IF NO WEIGHTS FILE TO USE, THEN JUST LEAVE ARG OUT. THE LAOD FUNCTION WILL KNOW TO START ANEW\n",
        "# Checkpoint callback saves into filepath=os.path.join(training_results,\"vae_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\")\n",
        "weights = args.weights\n",
        "try:\n",
        "  weights = os.path.join(training_results, weights)\n",
        "except TypeError:\n",
        "  pass\n",
        "\n",
        "#annealing_rate = args.klannealrate\n",
        "#intermediate_dim = args.intermediatedim\n",
        "#total_num_epochs = args.epochs\n",
        "#batch_size = args.batchsize\n",
        "\n",
        "\n",
        "\n",
        "print(root)\n",
        "print(input_data)\n",
        "print(training_results)\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COMP700_Honours Project\n",
            "/content/drive/MyDrive/COMP700_Honours Project/Data/remove_low_interaction_users/split/matrices/implicit\n",
            "/content/drive/MyDrive/COMP700_Honours Project/training_results\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8A_Q0_gVfR"
      },
      "source": [
        "#Model design\n",
        "* sample layer\n",
        "* batch history callback\n",
        "* vae loss function\n",
        "* VAE builder class\n",
        "* data generator sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5rKmGkUo_V"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, name=\"Sampling\", **kwargs):\n",
        "    super(Sampling, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "\n",
        "\n",
        "    #epsilon = distribution.sample()\n",
        "    \n",
        "    epsilon = tfback.random_normal(shape=(batch,dim))\n",
        "    sample = epsilon * tf.exp(0.5 * z_log_var)  +   z_mean  #Reparametrization trick: convert from standard normal to desired distribution\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvcmpfPd3Sg"
      },
      "source": [
        "class BatchHistory(keras.callbacks.Callback):  \n",
        "  def __init__(self, pel, pbl, pevl):\n",
        "    super(BatchHistory,self).__init__() \n",
        "    self.loss = [] \n",
        "    self.val_loss = 0.0\n",
        "    self.pel = pel\n",
        "    self.pbl = pbl\n",
        "    self.pevl = pevl\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):   \n",
        "    self.loss.append(logs.get('loss'))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.val_loss = logs.get('val_loss')\n",
        "    self.loss = [str(i) for i in self.loss]\n",
        "\n",
        "    with open(os.path.join(training_results,self.pevl), \"a\") as pe_v_loss:\n",
        "      pe_v_loss.write('\\n'+ str(self.val_loss))\n",
        "\n",
        "    with open(os.path.join(training_results,self.pbl), \"a\") as pb_loss:\n",
        "      pb_loss.write(\"\\n\")\n",
        "      pb_loss.writelines('\\n'.join(self.loss))\n",
        "\n",
        "    with open(os.path.join(training_results,self.pel), \"a\") as pe_loss:\n",
        "      pe_loss.write('\\n'+ str(self.loss[len(self.loss)-1]))\n",
        "    self.loss = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mHDOrkYFlK"
      },
      "source": [
        "#original_dim = 127350\n",
        "annealing_rate = args.klannealrate\n",
        "KLBeta = 0\n",
        "def VAE_loss(y_true, y_pred):\n",
        "    global KLBeta\n",
        "    reconst_loss =  original_dim * losses.binary_crossentropy(y_true, y_pred)\n",
        "    KLDiv = KLBeta * losses.kl_divergence(y_true, y_pred)\n",
        "\n",
        "    KLBeta = min(KLBeta+annealing_rate , 1) #update weight of KL factor\n",
        "\n",
        "    #return (KLDiv,reconst_loss)\n",
        "\n",
        "    return  reconst_loss + KLDiv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_ca8PXgjYB"
      },
      "source": [
        "class vae_builder(object):\n",
        "  def __init__(self, original_dim, intermediate_dim, latent_dim, name='VAE'):\n",
        "    self.name = name\n",
        "    self.original_dim = original_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  \n",
        "  def build(self):\n",
        "    self.input = layers.Input(self.original_dim, name = 'input')\n",
        "    self.dropout = layers.Dropout(rate=0.5)(self.input)\n",
        "\n",
        "    #encoder\n",
        "    self.e1 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e1')(self.dropout)\n",
        "    self.e2 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e2')(self.e1)\n",
        "    self.e3 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e3')(self.e2)\n",
        "    self.e4 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e4')(self.e3)\n",
        "    self.e5 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e5')(self.e4)\n",
        "\n",
        "    #sampling\n",
        "    self.mean = layers.Dense(self.latent_dim, name = 'mean')(self.e5)  \n",
        "    self.log_var = layers.Dense(self.latent_dim, name = 'log_var')(self.e5)  \n",
        "    self.sampling = Sampling()([self.mean, self.log_var])\n",
        "\n",
        "    #decoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='relu',name = 'd1')(self.sampling)\n",
        "    self.d2 = layers.Dense(self.original_dim, activation='relu', name = 'd2')(self.d1)  \n",
        "    \n",
        "    self.output = layers.Activation(activations.swish, name = 'output')(self.d2)\n",
        "\n",
        "    #to model\n",
        "    vae = Model(inputs = self.input, outputs = self.output, name = self.name)\n",
        "    return vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjfLEDZeTr2"
      },
      "source": [
        "class datagen(keras.utils.Sequence):\n",
        "  def __init__(self, x_set, batch_size, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "  def __len__(self):\n",
        "      #return math.ceil((self.x.shape[0]) / self.batch_size)\n",
        "      return len(self.shuffled_idx)\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "\n",
        "    batch_y = np.array(self.x[idx].todense())\n",
        "\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNmS-DzMgZRC"
      },
      "source": [
        "#Primary Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZbIuw34Dxx"
      },
      "source": [
        "##load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMzOLXXrXLN"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"train_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aYySDmfRAF"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]\n",
        "intermediate_dim = args.intermediatedim\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "\n",
        "total_num_epochs = args.epochs\n",
        "\n",
        "batch_size = args.batchsize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ia7Da3rY8a"
      },
      "source": [
        "train_gen = datagen(train , batch_size, 10)\n",
        "vad_gen = datagen(vad ,  batch_size, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-AG8m04TBI"
      },
      "source": [
        "##code to allow continuation of training over many sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mlp-VtwfeyE"
      },
      "source": [
        "def load_model(v, weights_file):\n",
        "  try:\n",
        "    v.load_weights(weights_file)\n",
        "    print(\"weights loaded successfully\")\n",
        "  except:\n",
        "    print(\"failed to load weights\")\n",
        "\n",
        "def get_num_epochs_complete():\n",
        "  try:\n",
        "    with open(os.path.join(training_results,args.prim_pevl), \"r\") as f:\n",
        "      lines = [i for i in f.readlines() if len(i.strip()) > 0]\n",
        "      return len(lines)\n",
        "  except FileNotFoundError:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzvqmDzw4g5_"
      },
      "source": [
        "##Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ugK83oxrQkA"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_primary_train_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\"), verbose=1, save_best_only=False)\n",
        "batch_hist = BatchHistory(args.prim_pel, args.prim_pbl, args.prim_pevl) #use primary train loss logs\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "#configurable optimiser\n",
        "slow_adam = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "\n",
        "v.compile(optimizer=slow_adam, loss=VAE_loss)\n",
        "\n",
        "#PICK UP FROM WHERE LEFT OFF\n",
        "complete_epochs = get_num_epochs_complete()\n",
        "remaining_epochs = total_num_epochs - complete_epochs\n",
        "load_model(v, weights)\n",
        "\n",
        "#RUN\n",
        "print(\"commisioned to run for\",total_num_epochs,\"epochs total\")\n",
        "print(\"known loss values found for \",complete_epochs,\"previous epochs\")\n",
        "print(\"will run for\", remaining_epochs, \"more epochs unless stopped early\")\n",
        "\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist])\n",
        "\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}