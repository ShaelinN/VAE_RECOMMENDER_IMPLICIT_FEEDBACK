{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gbFjVqXpgQ-E",
        "GZ8A_Q0_gVfR",
        "dNmS-DzMgZRC",
        "XFx-DlHv0bNW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0bWJr20k5J1"
      },
      "source": [
        "# !pip3 install tensorflow-ranking\n",
        "# !pip3 install tensorflow\n",
        "# !pip3 install numpy\n",
        "# !pip3 install pickle\n",
        "# !pip3 install math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aj-nLb6kRj5",
        "outputId": "f94ee6b4-e0d8-43af-f1f5-f9ce9a0bc14b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFjVqXpgQ-E"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rNgvglTWCN"
      },
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as tfback\n",
        "from tensorflow.keras import layers, activations, Model, losses\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import math\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pRl4vBS7LM"
      },
      "source": [
        "\"\"\"\n",
        "#uncomment if running as .py on cluster such as HIPPO\n",
        "#comment if running as .ipynb on Google Colab\n",
        "\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input_data', type=str)\n",
        "parser.add_argument('--training_results', type=str, default=\"vae_ml10m_training_results\")\n",
        "\n",
        "parser.add_argument('--prim_pel', type=str, default=\"prim_pel.txt\")\n",
        "parser.add_argument('--prim_pbl', type=str, default=\"prim_pbl.txt\")\n",
        "parser.add_argument('--prim_pevl', type=str, default=\"prim_pevl.txt\")\n",
        "\n",
        "parser.add_argument('--sec_pel', type=str, default=\"sec_pel.txt\")\n",
        "parser.add_argument('--sec_pbl', type=str, default=\"sec_pbl.txt\")\n",
        "parser.add_argument('--sec_pevl', type=str, default=\"sec_pevl.txt\")\n",
        "\n",
        "\n",
        "parser.add_argument('--weights', type=str)\n",
        "parser.add_argument('--intermediatedim', type=int, default=512)\n",
        "parser.add_argument('--epochs', type=int, default=30)\n",
        "parser.add_argument('--batchsize', type=int, default=500)\n",
        "parser.add_argument('--klannealrate', type=float, default=0.001)\n",
        "args = parser.parse_args()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akM7uK8m5Jw3"
      },
      "source": [
        "#uncomment if running as .ipynb on Google Colabas\n",
        "#comment if running as  .py on cluster such as HIPPO\n",
        "class argclass(object):\n",
        "  def __init__(self):\n",
        "    self.root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "    self.input_data = \"Data/movielens_10m/\"\n",
        "    self.training_results =\"vae_ml10m_training_results\"\n",
        "\n",
        "    self.prim_pel = \"prim_pel.txt\"        #primary training, per-epoch loss\n",
        "    self.prim_pbl = \"prim_pbl.txt\"        #primary training, per-batch loss\n",
        "    self.prim_pevl = \"prim_pevl.txt\"      #primary training, per-epoch validation loss\n",
        "\n",
        "    self.sec_pel = \"sec_pel.txt\"        #secondary training, per-epoch loss\n",
        "    self.sec_pbl = \"sec_pbl.txt\"        #secondary training, per-batch loss\n",
        "    self.sec_pevl = \"sec_pevl.txt\"      #secondary training, per-epoch validation loss\n",
        "\n",
        "    self.weights = \"vae_primary_train_best.hdf5\"\n",
        "    self.intermediatedim = 512 \n",
        "    self.epochs = 30\n",
        "    self.batchsize = 500\n",
        "    self.klannealrate = 0.001\n",
        "args = argclass()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTtzkG8xR_zO",
        "outputId": "95a20ce6-3e95-4d84-b881-baa9979af50a"
      },
      "source": [
        "root = args.root\n",
        "\n",
        "input_data = args.input_data\n",
        "input_data = os.path.join(root, input_data, \"split/matrices/implicit\")\n",
        "\n",
        "training_results = args.training_results\n",
        "training_results = os.path.join(root, training_results)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_results)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "#IF NO WEIGHTS FILE TO USE, THEN JUST LEAVE ARG OUT. THE LAOD FUNCTION WILL KNOW TO START ANEW\n",
        "# Checkpoint callback saves into filepath=os.path.join(training_results,\"vae_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\")\n",
        "weights = args.weights\n",
        "try:\n",
        "  weights = os.path.join(training_results, weights)\n",
        "except TypeError:\n",
        "  pass\n",
        "\n",
        "#annealing_rate = args.klannealrate\n",
        "#intermediate_dim = args.intermediatedim\n",
        "#total_num_epochs = args.epochs\n",
        "#batch_size = args.batchsize\n",
        "\n",
        "\n",
        "\n",
        "print(root)\n",
        "print(input_data)\n",
        "print(training_results)\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COMP700_Honours Project\n",
            "/content/drive/MyDrive/COMP700_Honours Project/Data/movielens/split/matrices/implicit\n",
            "/content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results\n",
            "/content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8A_Q0_gVfR"
      },
      "source": [
        "#Model design\n",
        "* sample layer\n",
        "* batch history callback\n",
        "* vae loss function\n",
        "* VAE builder class\n",
        "* data generator sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5rKmGkUo_V"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, name=\"Sampling\", **kwargs):\n",
        "    super(Sampling, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "\n",
        "\n",
        "    #epsilon = distribution.sample()\n",
        "    \n",
        "    epsilon = tfback.random_normal(shape=(batch,dim))\n",
        "    sample = epsilon * tf.exp(0.5 * z_log_var)  +   z_mean  #Reparametrization trick: convert from standard normal to desired distribution\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvcmpfPd3Sg"
      },
      "source": [
        "class BatchHistory(keras.callbacks.Callback):  \n",
        "  def __init__(self, pel, pbl, pevl):\n",
        "    super(BatchHistory,self).__init__() \n",
        "    self.loss = [] \n",
        "    self.val_loss = 0.0\n",
        "    self.pel = pel\n",
        "    self.pbl = pbl\n",
        "    self.pevl = pevl\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):   \n",
        "    self.loss.append(logs.get('loss'))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.val_loss = logs.get('val_loss')\n",
        "    self.loss = [str(i) for i in self.loss]\n",
        "\n",
        "    with open(os.path.join(training_results,self.pevl), \"a\") as pe_v_loss:\n",
        "      pe_v_loss.write('\\n'+ str(self.val_loss))\n",
        "\n",
        "    with open(os.path.join(training_results,self.pbl), \"a\") as pb_loss:\n",
        "      pb_loss.write(\"\\n\")\n",
        "      pb_loss.writelines('\\n'.join(self.loss))\n",
        "\n",
        "    with open(os.path.join(training_results,self.pel), \"a\") as pe_loss:\n",
        "      pe_loss.write('\\n'+ str(self.loss[len(self.loss)-1]))\n",
        "    self.loss = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mHDOrkYFlK"
      },
      "source": [
        "#original_dim = 127350\n",
        "annealing_rate = args.klannealrate\n",
        "#KLBeta = 1 # initialised to relevant values(0 for prim, 1 for sec) immediately before each training section\n",
        "def VAE_loss(y_true, y_pred):\n",
        "    global KLBeta\n",
        "    reconst_loss =  original_dim * losses.binary_crossentropy(y_true, y_pred)\n",
        "    KLDiv = KLBeta * losses.kl_divergence(y_true, y_pred)\n",
        "\n",
        "    KLBeta = min(KLBeta+annealing_rate , 1) #update weight of KL factor\n",
        "\n",
        "    #return (KLDiv,reconst_loss)\n",
        "\n",
        "    return  reconst_loss + KLDiv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_ca8PXgjYB"
      },
      "source": [
        "class vae_builder(object):\n",
        "  def __init__(self, original_dim, intermediate_dim, latent_dim, name='VAE'):\n",
        "    self.name = name\n",
        "    self.original_dim = original_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  \n",
        "  def build(self):\n",
        "    self.input = layers.Input(self.original_dim, name = 'input')\n",
        "    #self.dropout = layers.Dropout(rate=0.5)(self.input)\n",
        "\n",
        "    #encoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_1')(self.input)\n",
        "    self.n1 = layers.LayerNormalization(name = 'encoder_layernorm_1')(self.d1)\n",
        "\n",
        "    self.d2 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_2')(self.n1)\n",
        "    self.n2 = layers.LayerNormalization(name = 'encoder_layernorm_2')(self.d2)\n",
        "\n",
        "    self.d3 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_3')(self.n2)\n",
        "    self.n3 = layers.LayerNormalization(name = 'encoder_layernorm_3')(self.d3)\n",
        "\n",
        "    self.d4 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_4')(self.n3)\n",
        "    self.n4 = layers.LayerNormalization(name = 'encoder_layernorm_4')(self.d4)\n",
        "\n",
        "    self.d5 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_5')(self.n4)\n",
        "    self.n5 = layers.LayerNormalization(name = 'encoder_layernorm_5')(self.d5)\n",
        "\n",
        "    #sampling\n",
        "    self.mean = layers.Dense(self.latent_dim, name = 'mean')(self.n5)  \n",
        "    self.log_var = layers.Dense(self.latent_dim, name = 'log_var')(self.n5)  \n",
        "    self.sampling = Sampling()([self.mean, self.log_var])\n",
        "\n",
        "    #decoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='relu',name = 'decoder_dense_1')(self.sampling)\n",
        "    self.d2 = layers.Dense(self.original_dim, activation='relu', name = 'decoder_dense_2')(self.d1)  \n",
        "    \n",
        "    self.output = layers.Activation(activations.swish, name = 'output')(self.d2)\n",
        "\n",
        "    #to model\n",
        "    vae = Model(inputs = self.input, outputs = self.output, name = self.name)\n",
        "    return vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjfLEDZeTr2"
      },
      "source": [
        "class datagen(keras.utils.Sequence):\n",
        "  def __init__(self, x_set, y_set, batch_size=500, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.y = y_set\n",
        "  \n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def __len__(self):\n",
        "      return math.ceil((self.shuffled_idx.shape[0]) / self.batch_size)\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "    batch_y = np.array(self.y[idx].todense())\n",
        "\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grY5hZIY0wVz"
      },
      "source": [
        "#TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNLHxF_k0u64"
      },
      "source": [
        "intermediate_dim = args.intermediatedim\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "\n",
        "total_num_epochs = args.epochs\n",
        "\n",
        "batch_size = args.batchsize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-AG8m04TBI"
      },
      "source": [
        "###code to allow continuation of training over many sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mlp-VtwfeyE"
      },
      "source": [
        "def load_model(v, weights_file):\n",
        "  try:\n",
        "    v.load_weights(weights_file)\n",
        "    print(\"weights loaded successfully\")\n",
        "  except:\n",
        "    print(\"failed to load weights\")\n",
        "\n",
        "def get_num_epochs_complete():\n",
        "  try:\n",
        "    with open(os.path.join(training_results,args.prim_pevl), \"r\") as f:\n",
        "      lines = [i for i in f.readlines() if len(i.strip()) > 0]\n",
        "      return len(lines)\n",
        "  except FileNotFoundError:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNmS-DzMgZRC"
      },
      "source": [
        "##Primary Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZbIuw34Dxx"
      },
      "source": [
        "###load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMzOLXXrXLN"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"train_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aYySDmfRAF"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ia7Da3rY8a",
        "outputId": "d76f511a-4535-48d7-9d75-3eec0b0d1f40"
      },
      "source": [
        "train_gen = datagen(train,train , batch_size)\n",
        "vad_gen = datagen(vad,vad,  batch_size)\n",
        "train_gen.__len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otk59UumZidI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf80123a-917d-4502-cae4-d4916428fa4a"
      },
      "source": [
        "a = train_gen.__getitem__(0)[0][0]\n",
        "b = np.nonzero(a)[0]\n",
        "print(b)\n",
        "\n",
        "print(a[b])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   83   831  1633  1676  1895  2070  2117  2492  2621  2630  2691  2789\n",
            "  2835  2872  2886  3018  3779  4103  4114  4647  4725  5127  5562  6388\n",
            "  6815  7131  7341  7469  7473  8100  8239  8240  8751  8885  9098  9443\n",
            "  9461  9463  9673 10398 10495 10608 10678 10983 11056 11243 11312 11804\n",
            " 12121 12272 12583 12705 13110 13136 13709 14841 14884 14937 14954 15133\n",
            " 15174 15400 15845 15851 16083 16483 16932 17613 17664 17710 17711]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzvqmDzw4g5_"
      },
      "source": [
        "###Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ugK83oxrQkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9003effe-87fa-49ff-a982-dd3d5b13dab6"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "KLBeta = 0\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_primary_train_best.hdf5\"), verbose=1, save_best_only=True)\n",
        "batch_hist = BatchHistory(args.prim_pel, args.prim_pbl, args.prim_pevl) #use primary train loss logs\n",
        "reduce_LR = ReduceLROnPlateau(patience=10,verbose=1, factor=0.5)\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "v.compile(optimizer='adam', loss=VAE_loss)\n",
        "\n",
        "#PICK UP FROM WHERE LAST LEFT OFF\n",
        "complete_epochs = get_num_epochs_complete()\n",
        "remaining_epochs = total_num_epochs - complete_epochs\n",
        "load_model(v, weights)\n",
        "\n",
        "#RUN\n",
        "print(\"commisioned to run for\",total_num_epochs,\"epochs total\")\n",
        "print(\"known loss values found for \",complete_epochs,\"previous epochs\")\n",
        "print(\"will run for\", remaining_epochs, \"more epochs unless stopped early\")\n",
        "\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist, reduce_LR])\n",
        "\n",
        "print(\"complete\")\n",
        "\n",
        "\n",
        "keras.utils.plot_model(v, show_layer_activations=True)\n",
        "v.save_weights(os.path.join(training_results,\"vae_primary_train_last.hdf5\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed to load weights\n",
            "commisioned to run for 30 epochs total\n",
            "known loss values found for  0 previous epochs\n",
            "will run for 30 more epochs unless stopped early\n",
            "Epoch 1/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 1095.0676\n",
            "Epoch 00001: val_loss improved from inf to 1002.87231, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 10s 86ms/step - loss: 1095.0676 - val_loss: 1002.8723 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 929.0750\n",
            "Epoch 00002: val_loss improved from 1002.87231 to 916.62769, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 11s 106ms/step - loss: 929.0750 - val_loss: 916.6277 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 897.6243\n",
            "Epoch 00003: val_loss improved from 916.62769 to 883.40692, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 12s 118ms/step - loss: 897.6243 - val_loss: 883.4069 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 869.2576\n",
            "Epoch 00004: val_loss improved from 883.40692 to 867.72400, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 869.2576 - val_loss: 867.7240 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 827.8147\n",
            "Epoch 00005: val_loss improved from 867.72400 to 823.20557, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 9s 89ms/step - loss: 827.8147 - val_loss: 823.2056 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 817.1251\n",
            "Epoch 00006: val_loss improved from 823.20557 to 807.16998, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 9s 87ms/step - loss: 817.1251 - val_loss: 807.1700 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 778.5114\n",
            "Epoch 00007: val_loss improved from 807.16998 to 785.92743, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 15s 152ms/step - loss: 778.5114 - val_loss: 785.9274 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 790.3742\n",
            "Epoch 00008: val_loss did not improve from 785.92743\n",
            "100/100 [==============================] - 8s 80ms/step - loss: 790.3742 - val_loss: 796.6384 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 777.7324\n",
            "Epoch 00009: val_loss did not improve from 785.92743\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 777.7324 - val_loss: 786.7463 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 780.1364\n",
            "Epoch 00010: val_loss improved from 785.92743 to 780.32538, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 10s 102ms/step - loss: 780.1364 - val_loss: 780.3254 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 762.1924\n",
            "Epoch 00011: val_loss improved from 780.32538 to 774.17322, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 84ms/step - loss: 762.1924 - val_loss: 774.1732 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 758.4879\n",
            "Epoch 00012: val_loss did not improve from 774.17322\n",
            "100/100 [==============================] - 8s 82ms/step - loss: 758.4879 - val_loss: 775.1724 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 771.8878\n",
            "Epoch 00013: val_loss did not improve from 774.17322\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 771.8878 - val_loss: 782.0930 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 769.2619\n",
            "Epoch 00014: val_loss did not improve from 774.17322\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 769.2619 - val_loss: 782.0309 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 786.4769\n",
            "Epoch 00015: val_loss did not improve from 774.17322\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 786.4769 - val_loss: 781.3873 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 763.1954\n",
            "Epoch 00016: val_loss improved from 774.17322 to 773.13892, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 12s 120ms/step - loss: 763.1954 - val_loss: 773.1389 - lr: 0.0010\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 785.4365\n",
            "Epoch 00017: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 785.4365 - val_loss: 793.4621 - lr: 0.0010\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 838.8027\n",
            "Epoch 00018: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 81ms/step - loss: 838.8027 - val_loss: 929.4340 - lr: 0.0010\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 809.7079\n",
            "Epoch 00019: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 809.7079 - val_loss: 785.9484 - lr: 0.0010\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 764.4727\n",
            "Epoch 00020: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 764.4727 - val_loss: 780.9522 - lr: 0.0010\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 768.6141\n",
            "Epoch 00021: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 768.6141 - val_loss: 779.7805 - lr: 0.0010\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 767.1725\n",
            "Epoch 00022: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 767.1725 - val_loss: 778.5649 - lr: 0.0010\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 761.5173\n",
            "Epoch 00023: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 761.5173 - val_loss: 774.7852 - lr: 0.0010\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 760.4387\n",
            "Epoch 00024: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 75ms/step - loss: 760.4387 - val_loss: 774.4870 - lr: 0.0010\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 858.1494\n",
            "Epoch 00025: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 858.1494 - val_loss: 882.1168 - lr: 0.0010\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 835.5067\n",
            "Epoch 00026: val_loss did not improve from 773.13892\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 835.5067 - val_loss: 841.6006 - lr: 0.0010\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 824.6691\n",
            "Epoch 00027: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 824.6691 - val_loss: 838.7184 - lr: 5.0000e-04\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 822.3391\n",
            "Epoch 00028: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 822.3391 - val_loss: 836.7725 - lr: 5.0000e-04\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 820.7087\n",
            "Epoch 00029: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 820.7087 - val_loss: 835.3816 - lr: 5.0000e-04\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 819.5131\n",
            "Epoch 00030: val_loss did not improve from 773.13892\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 819.5131 - val_loss: 834.3265 - lr: 5.0000e-04\n",
            "complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFx-DlHv0bNW"
      },
      "source": [
        "##Secondary Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13aCtS771N-M"
      },
      "source": [
        "###load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwk_dMrd1N-T"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_training_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvwe_I2L1N-U"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9XTqDk51N-U",
        "outputId": "5f5dec48-d423-47ef-ecbd-e08c91eea954"
      },
      "source": [
        "train_gen = datagen(train, train , batch_size)\n",
        "vad_gen = datagen(vad , vad, batch_size)\n",
        "train_gen.__len__()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTQKPu11N-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1851d6-6596-40e5-9127-40319977ed1c"
      },
      "source": [
        "a = train_gen.__getitem__(1)[0][0]\n",
        "b = np.nonzero(a)[0]\n",
        "print(b)\n",
        "\n",
        "print(a[b])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1813 3665 5533 6886 8110 9199]\n",
            "[1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inl2bnFt1N-V"
      },
      "source": [
        "###Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvjfHrmS1N-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79916e2d-1472-4cb6-cd44-30020d35b53c"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "KLBeta = 1\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_secondary_train_best.hdf5\"), verbose=1, save_best_only=True)\n",
        "batch_hist = BatchHistory(args.sec_pel, args.sec_pbl, args.sec_pevl) #use secondary train loss logs\n",
        "reduce_LR = ReduceLROnPlateau(patience=10,verbose=1, factor=0.5)\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "v.compile(optimizer='adam', loss=VAE_loss)\n",
        "\n",
        "remaining_epochs = 10\n",
        "v.load_weights(os.path.join(training_results,\"vae_primary_train_best.hdf5\"))#load best weights from primary\n",
        "\n",
        "#RUN\n",
        "print(\"will run for\", remaining_epochs, \"more epochs unless stopped early\")\n",
        "\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist, reduce_LR])\n",
        "\n",
        "print(\"complete\")\n",
        "\n",
        "v.save_weights(os.path.join(training_results,\"vae_secondary_train_last.hdf5\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will run for 10 more epochs unless stopped early\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 234.7955\n",
            "Epoch 00001: val_loss improved from inf to 1882.45422, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 6s 218ms/step - loss: 234.7955 - val_loss: 1882.4542 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 207.8086\n",
            "Epoch 00002: val_loss improved from 1882.45422 to 1846.05286, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 4s 219ms/step - loss: 207.8086 - val_loss: 1846.0529 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 191.6955\n",
            "Epoch 00003: val_loss did not improve from 1846.05286\n",
            "20/20 [==============================] - 2s 121ms/step - loss: 191.6955 - val_loss: 2436.9902 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 189.0064\n",
            "Epoch 00004: val_loss improved from 1846.05286 to 1790.70703, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 3s 152ms/step - loss: 189.0064 - val_loss: 1790.7070 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 185.8319\n",
            "Epoch 00005: val_loss improved from 1790.70703 to 1694.40845, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 5s 246ms/step - loss: 185.8319 - val_loss: 1694.4084 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 193.2461\n",
            "Epoch 00006: val_loss did not improve from 1694.40845\n",
            "20/20 [==============================] - 3s 139ms/step - loss: 193.2461 - val_loss: 1848.1075 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 187.3252\n",
            "Epoch 00007: val_loss did not improve from 1694.40845\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 187.3252 - val_loss: 1869.6121 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 185.5860\n",
            "Epoch 00008: val_loss did not improve from 1694.40845\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 185.5860 - val_loss: 1816.6044 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 185.3187\n",
            "Epoch 00009: val_loss did not improve from 1694.40845\n",
            "20/20 [==============================] - 3s 130ms/step - loss: 185.3187 - val_loss: 1830.2672 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 185.5036\n",
            "Epoch 00010: val_loss did not improve from 1694.40845\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 185.5036 - val_loss: 1810.3728 - lr: 0.0010\n",
            "complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxZMmBctKkUC"
      },
      "source": [
        "#Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "BOw3gzaFoWhI",
        "outputId": "f3197a18-270a-42bf-c077-fcb96a74f205"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "relevant_loss_data = args.prim_pel\n",
        "\n",
        "loss_file = os.path.join(args.root, args.training_results, relevant_loss_data)\n",
        "with open (loss_file, \"r\") as f:\n",
        "  #a = f.readlines()\n",
        "  a = np.array([i.strip() for i in f.readlines()][1:]).astype(np.float)\n",
        "  a = [float(i) for i in a]\n",
        "\n",
        "print(len(a))\n",
        "l = [np.inf]\n",
        "l.extend(a)\n",
        "f = plt.plot(l)\n",
        "plt.ylim(0, 1200)\n",
        "\n",
        "plt.xticks(np.arange(0, 31, 5))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f14e2260350>,\n",
              "  <matplotlib.axis.XTick at 0x7f14e2260e10>,\n",
              "  <matplotlib.axis.XTick at 0x7f14e1d61f90>,\n",
              "  <matplotlib.axis.XTick at 0x7f14e1d1c550>,\n",
              "  <matplotlib.axis.XTick at 0x7f14e1d1ca90>,\n",
              "  <matplotlib.axis.XTick at 0x7f14e1d1c690>,\n",
              "  <matplotlib.axis.XTick at 0x7f14e1d24410>],\n",
              " <a list of 7 Text major ticklabel objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMBz6U4etcCA",
        "outputId": "58f16f78-69a8-4d27-8269-a191be3c7b44"
      },
      "source": [
        "v.load_weights(weights)\n",
        "\n",
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"train_impl.pkl\") , 'rb')\n",
        ")\n",
        "train_gen = datagen(train,train , args.batch_size)\n",
        "\n",
        "\n",
        "batch_num = 0\n",
        "item_num = 0\n",
        "\n",
        "ins = np.array([train_gen.__getitem__(batch_num)[0][item_num]])\n",
        "a = v.predict(ins)\n",
        "\n",
        "a = -np.sort(-a)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hJ037Rc6PEa"
      },
      "source": [
        "*MOVIELENS*: \n",
        "\n",
        "After filtering, there are 9998816 events from 69878 users and 10196 businesses (sparsity: 1.403%)\n",
        "\n",
        "sample prediction: [[0.43879703, 0.4058009 , 0.4028352 , ..., 0.        , 0.        ,\n",
        "        0.        ]]\n",
        "\n",
        "\n",
        "*YELP*:\n",
        "\n",
        "(b only filter) After filtering, there are 8635403 events from 2189457 users and 160585 businesses (sparsity: 0.002%)\n",
        "\n",
        "(u only filter) After filtering, there are 5766970 events from 365665 users and 159108 businesses (sparsity: 0.010%)\n",
        "\n",
        "(full filter) After filtering, there are 5674527 events from 365664 users and 127351 businesses (sparsity: 0.012%)\n",
        "\n",
        "sample prediction: [[0., 0., 0., ..., 0., 0., 0.]]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rMjapfG449"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8rn6AEzLTU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53047d4e-dcf4-447b-93a0-0fbd40a2d969"
      },
      "source": [
        "#!pip3 install tensorflow-ranking\n",
        "\n",
        "import tensorflow_ranking as tfr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-ranking\n",
            "  Downloading tensorflow_ranking-0.5.0-py2.py3-none-any.whl (141 kB)\n",
            "\u001b[?25l\r\u001b[K     |                             | 10 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |                           | 20 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |                         | 30 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |                      | 40 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |                    | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |                  | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |               | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |             | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |           | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |        | 102 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |      | 112 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |    | 122 kB 5.6 MB/s eta 0:00:01\r\u001b[K     | | 133 kB 5.6 MB/s eta 0:00:01\r\u001b[K     || 141 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.15.0)\n",
            "Collecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
            "  Downloading tensorflow_serving_api-2.7.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tensorflow<3,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.41.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.13.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.22.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.10.0.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.37.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (12.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.3.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.6.0)\n",
            "Installing collected packages: tensorflow-serving-api, tensorflow-ranking\n",
            "Successfully installed tensorflow-ranking-0.5.0 tensorflow-serving-api-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h8RqAxNG196"
      },
      "source": [
        "eval_results = os.path.join(root, \"eval_results\")\n",
        "try:\n",
        "  os.mkdir(eval_results)\n",
        "except FileExistsError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r83NUj7HqeS"
      },
      "source": [
        "test_true = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_impl.pkl\") , 'rb')\n",
        ")\n",
        "\n",
        "test_given = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_testing_impl.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwqIdmTHqeY"
      },
      "source": [
        "total_num_samples = test_given.shape[0]\n",
        "original_dim = test_given.shape[1]\n",
        "intermediate_dim = args.intermediatedim\n",
        "latent_dim = intermediate_dim//2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vK4GPHbHqeZ",
        "outputId": "459a8ce9-d11b-4ac7-b7e6-f0da4add767d"
      },
      "source": [
        "test_gen = datagen(test_given ,test_true, args.batchsize)\n",
        "test_gen.__len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRKYSf8-H7M7"
      },
      "source": [
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "KLBeta = 1\n",
        "metrics = [ tfr.keras.metrics.RecallMetric(name = \"recall_20\", topn=20,),\n",
        "           tfr.keras.metrics.RecallMetric(name = \"recall_50\", topn=50,),\n",
        "           tfr.keras.metrics.NDCGMetric(name = \"ndcg_100\", topn=100,)         \n",
        "]\n",
        "\n",
        "\n",
        "v.compile(optimizer='adam', loss=VAE_loss, metrics = metrics)\n",
        "v.load_weights(os.path.join(training_results,\"vae_secondary_train_best.hdf5\"))#load best weights from secondary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Dn-xWNIO3r",
        "outputId": "e8d294dd-11a0-4f82-8969-dc40090e1191"
      },
      "source": [
        "preds = v.evaluate(test_gen, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 3s 106ms/step - loss: 1658.5330 - recall_20: 0.0691 - recall_50: 0.1480 - ndcg_100: 0.3272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HIGfZkU_Ip-W",
        "outputId": "6d7103ba-b47c-4af8-e19f-24d441ab3f25"
      },
      "source": [
        "print(preds)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "#yelp:\n",
        "0.00011902114056283608\n",
        "0.0003276122151874006\n",
        "0.00030501390574499965\n",
        "\n",
        "#ml10m\n",
        "0.06914284080266953, \n",
        "0.1480351686477661, \n",
        "0.32721519470214844\n",
        "\n",
        "ml20m\n",
        "0.0676\n",
        "0.1382\n",
        "0.3121\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1658.532958984375, 0.06914284080266953, 0.1480351686477661, 0.32721519470214844]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#yelp:\\n0.00011902114056283608\\n0.0003276122151874006\\n0.00030501390574499965\\n\\n#ml10m\\n0.06898434460163116, \\n0.1480257213115692, \\n0.32717984914779663\\n\\nml20m\\n\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}