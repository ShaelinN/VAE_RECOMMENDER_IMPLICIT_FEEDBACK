{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GZ8A_Q0_gVfR",
        "grY5hZIY0wVz",
        "XFx-DlHv0bNW",
        "rxZMmBctKkUC"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0bWJr20k5J1"
      },
      "source": [
        "# !pip3 install tensorflow-ranking\n",
        "# !pip3 install tensorflow\n",
        "# !pip3 install numpy\n",
        "# !pip3 install pickle\n",
        "# !pip3 install math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aj-nLb6kRj5",
        "outputId": "fbea4cf0-112d-47d6-d07a-7a53cf24bd23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFjVqXpgQ-E"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rNgvglTWCN"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as tfback\n",
        "from tensorflow.keras import layers, activations, Model, losses\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pRl4vBS7LM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b50191f1-6f2f-48c1-cd49-7f53f1a3f521"
      },
      "source": [
        "\"\"\"\n",
        "#uncomment if running as .py on cluster such as HIPPO\n",
        "#comment if running as .ipynb on Google Colab\n",
        "\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input_data', type=str)\n",
        "parser.add_argument('--training_results', type=str, default=\"vae_ml10m_training_results\")\n",
        "\n",
        "parser.add_argument('--prim_pel', type=str, default=\"prim_pel.txt\")\n",
        "parser.add_argument('--prim_pbl', type=str, default=\"prim_pbl.txt\")\n",
        "parser.add_argument('--prim_pevl', type=str, default=\"prim_pevl.txt\")\n",
        "\n",
        "parser.add_argument('--sec_pel', type=str, default=\"sec_pel.txt\")\n",
        "parser.add_argument('--sec_pbl', type=str, default=\"sec_pbl.txt\")\n",
        "parser.add_argument('--sec_pevl', type=str, default=\"sec_pevl.txt\")\n",
        "\n",
        "\n",
        "parser.add_argument('--weights', type=str)\n",
        "parser.add_argument('--intermediatedim', type=int, default=512)\n",
        "parser.add_argument('--epochs', type=int, default=30)\n",
        "parser.add_argument('--batchsize', type=int, default=500)\n",
        "parser.add_argument('--klannealrate', type=float, default=0.001)\n",
        "args = parser.parse_args()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#uncomment if running as .py on cluster such as HIPPO\\n#comment if running as .ipynb on Google Colab\\n\\n\\nimport argparse\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\'--root\\', type=str)\\nparser.add_argument(\\'--input_data\\', type=str)\\nparser.add_argument(\\'--training_results\\', type=str, default=\"vae_ml10m_training_results\")\\n\\nparser.add_argument(\\'--prim_pel\\', type=str, default=\"prim_pel.txt\")\\nparser.add_argument(\\'--prim_pbl\\', type=str, default=\"prim_pbl.txt\")\\nparser.add_argument(\\'--prim_pevl\\', type=str, default=\"prim_pevl.txt\")\\n\\nparser.add_argument(\\'--sec_pel\\', type=str, default=\"sec_pel.txt\")\\nparser.add_argument(\\'--sec_pbl\\', type=str, default=\"sec_pbl.txt\")\\nparser.add_argument(\\'--sec_pevl\\', type=str, default=\"sec_pevl.txt\")\\n\\n\\nparser.add_argument(\\'--weights\\', type=str)\\nparser.add_argument(\\'--intermediatedim\\', type=int, default=512)\\nparser.add_argument(\\'--epochs\\', type=int, default=30)\\nparser.add_argument(\\'--batchsize\\', type=int, default=500)\\nparser.add_argument(\\'--klannealrate\\', type=float, default=0.001)\\nargs = parser.parse_args()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akM7uK8m5Jw3"
      },
      "source": [
        "#uncomment if running as .ipynb on Google Colabas\n",
        "#comment if running as  .py on cluster such as HIPPO\n",
        "class argclass(object):\n",
        "  def __init__(self):\n",
        "    self.root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "\n",
        "    \n",
        "    self.input_data = \"Data/movielens_10m/\"\n",
        "    self.training_results =\"vae_ml10m_training_results\"\n",
        "\n",
        "    self.prim_pel = \"prim_pel.txt\"        #primary training, per-epoch loss\n",
        "    self.prim_pbl = \"prim_pbl.txt\"        #primary training, per-batch loss\n",
        "    self.prim_pevl = \"prim_pevl.txt\"      #primary training, per-epoch validation loss\n",
        "\n",
        "    self.sec_pel = \"sec_pel.txt\"        #secondary training, per-epoch loss\n",
        "    self.sec_pbl = \"sec_pbl.txt\"        #secondary training, per-batch loss\n",
        "    self.sec_pevl = \"sec_pevl.txt\"      #secondary training, per-epoch validation loss\n",
        "\n",
        "    self.weights = \"vae_primary_train_best.hdf5\"\n",
        "    self.intermediatedim = 512 \n",
        "    self.epochs = 30\n",
        "    self.batchsize = 500\n",
        "    self.klannealrate = 0.001\n",
        "args = argclass()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTtzkG8xR_zO",
        "outputId": "1679b5b2-e372-4196-e4ad-fa39aa3fa141"
      },
      "source": [
        "root = args.root\n",
        "\n",
        "input_data = args.input_data\n",
        "input_data = os.path.join(root, input_data, \"split/matrices/implicit\")\n",
        "\n",
        "training_results = args.training_results\n",
        "training_results = os.path.join(root, training_results)\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_results)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "#IF NO WEIGHTS FILE TO USE, THEN JUST LEAVE ARG OUT. THE LAOD FUNCTION WILL KNOW TO START ANEW\n",
        "# Checkpoint callback saves into filepath=os.path.join(training_results,\"vae_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\")\n",
        "weights = args.weights\n",
        "try:\n",
        "  weights = os.path.join(training_results, weights)\n",
        "except TypeError:\n",
        "  pass\n",
        "\n",
        "\n",
        "print(root)\n",
        "print(input_data)\n",
        "print(training_results)\n",
        "print(weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COMP700_Honours Project\n",
            "/content/drive/MyDrive/COMP700_Honours Project/Data/yelp/split/matrices/implicit\n",
            "/content/drive/MyDrive/COMP700_Honours Project/vae_y_training_results\n",
            "/content/drive/MyDrive/COMP700_Honours Project/vae_y_training_results/vae_primary_train_best.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNLHxF_k0u64"
      },
      "source": [
        "intermediate_dim = args.intermediatedim\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "\n",
        "total_num_epochs = args.epochs\n",
        "\n",
        "batch_size = args.batchsize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8A_Q0_gVfR"
      },
      "source": [
        "#Model design\n",
        "* sample layer\n",
        "* batch history callback\n",
        "* vae loss function\n",
        "* VAE builder class\n",
        "* data generator sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5rKmGkUo_V"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, name=\"Sampling\", **kwargs):\n",
        "    super(Sampling, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "\n",
        "\n",
        "    #epsilon = distribution.sample()\n",
        "    \n",
        "    epsilon = tfback.random_normal(shape=(batch,dim))\n",
        "    sample = epsilon * tf.exp(0.5 * z_log_var)  +   z_mean  #Reparametrization trick: convert from standard normal to desired distribution\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvcmpfPd3Sg"
      },
      "source": [
        "class BatchHistory(keras.callbacks.Callback):  \n",
        "  def __init__(self, pel, pbl, pevl):\n",
        "    super(BatchHistory,self).__init__() \n",
        "    self.loss = [] \n",
        "    self.val_loss = 0.0\n",
        "    self.pel = pel\n",
        "    self.pbl = pbl\n",
        "    self.pevl = pevl\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):   \n",
        "    self.loss.append(logs.get('loss'))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.val_loss = logs.get('val_loss')\n",
        "    self.loss = [str(i) for i in self.loss]\n",
        "\n",
        "    with open(os.path.join(training_results,self.pevl), \"a\") as pe_v_loss:\n",
        "      pe_v_loss.write('\\n'+ str(self.val_loss))\n",
        "\n",
        "    with open(os.path.join(training_results,self.pbl), \"a\") as pb_loss:\n",
        "      pb_loss.write(\"\\n\")\n",
        "      pb_loss.writelines('\\n'.join(self.loss))\n",
        "\n",
        "    with open(os.path.join(training_results,self.pel), \"a\") as pe_loss:\n",
        "      pe_loss.write('\\n'+ str(self.loss[len(self.loss)-1]))\n",
        "    self.loss = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mHDOrkYFlK"
      },
      "source": [
        "#original_dim = 127350\n",
        "annealing_rate = args.klannealrate\n",
        "#KLBeta = 1 # initialised to relevant values(0 for prim, 1 for sec) immediately before each training section\n",
        "def VAE_loss(y_true, y_pred):\n",
        "    global KLBeta\n",
        "    reconst_loss =  original_dim * losses.binary_crossentropy(y_true, y_pred)\n",
        "    KLDiv = KLBeta * losses.kl_divergence(y_true, y_pred)\n",
        "\n",
        "    KLBeta = min(KLBeta+annealing_rate , 1) #update weight of KL factor\n",
        "\n",
        "    #return (KLDiv,reconst_loss)\n",
        "\n",
        "    return  reconst_loss + KLDiv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_ca8PXgjYB"
      },
      "source": [
        "class vae_builder(object):\n",
        "  def __init__(self, original_dim, intermediate_dim, latent_dim, name='VAE'):\n",
        "    self.name = name\n",
        "    self.original_dim = original_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  \n",
        "  def build(self):\n",
        "    self.input = layers.Input(self.original_dim, name = 'input')\n",
        "    #self.dropout = layers.Dropout(rate=0.5)(self.input)\n",
        "\n",
        "    #encoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_1')(self.input)\n",
        "    self.n1 = layers.LayerNormalization(name = 'encoder_layernorm_1')(self.d1)\n",
        "\n",
        "    self.d2 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_2')(self.n1)\n",
        "    self.n2 = layers.LayerNormalization(name = 'encoder_layernorm_2')(self.d2)\n",
        "\n",
        "    self.d3 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_3')(self.n2)\n",
        "    self.n3 = layers.LayerNormalization(name = 'encoder_layernorm_3')(self.d3)\n",
        "\n",
        "    self.d4 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_4')(self.n3)\n",
        "    self.n4 = layers.LayerNormalization(name = 'encoder_layernorm_4')(self.d4)\n",
        "\n",
        "    self.d5 = layers.Dense(self.intermediate_dim, activation='tanh', name = 'encoder_dense_5')(self.n4)\n",
        "    self.n5 = layers.LayerNormalization(name = 'encoder_layernorm_5')(self.d5)\n",
        "\n",
        "    #sampling\n",
        "    self.mean = layers.Dense(self.latent_dim, name = 'mean')(self.n5)  \n",
        "    self.log_var = layers.Dense(self.latent_dim, name = 'log_var')(self.n5)  \n",
        "    self.sampling = Sampling()([self.mean, self.log_var])\n",
        "\n",
        "    #decoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='relu',name = 'decoder_dense_1')(self.sampling)\n",
        "    self.d2 = layers.Dense(self.original_dim, activation='relu', name = 'decoder_dense_2')(self.d1)  \n",
        "    \n",
        "    self.output = layers.Activation(activations.swish, name = 'output')(self.d2)\n",
        "\n",
        "    #to model\n",
        "    vae = Model(inputs = self.input, outputs = self.output, name = self.name)\n",
        "    return vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjfLEDZeTr2"
      },
      "source": [
        "class datagen(keras.utils.Sequence):\n",
        "  def __init__(self, x_set, y_set, batch_size=500, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.y = y_set\n",
        "    self.batch_size = batch_size\n",
        "    self.max_samples_per_epoch = max_samples_per_epoch\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "    if self.max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:self.max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def __len__(self):\n",
        "      return math.ceil((self.shuffled_idx.shape[0]) / self.batch_size)\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "    batch_y = np.array(self.y[idx].todense())\n",
        "\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grY5hZIY0wVz"
      },
      "source": [
        "#TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-AG8m04TBI"
      },
      "source": [
        "###code to allow continuation of training over many sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mlp-VtwfeyE"
      },
      "source": [
        "def load_model(v, weights_file):\n",
        "  try:\n",
        "    v.load_weights(weights_file)\n",
        "    print(\"weights loaded successfully\")\n",
        "  except:\n",
        "    print(\"failed to load weights\")\n",
        "\n",
        "def get_num_epochs_complete():\n",
        "  try:\n",
        "    with open(os.path.join(training_results,args.prim_pevl), \"r\") as f:\n",
        "      lines = [i for i in f.readlines() if len(i.strip()) > 0]\n",
        "      return len(lines)\n",
        "  except FileNotFoundError:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNmS-DzMgZRC"
      },
      "source": [
        "##Primary Train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZbIuw34Dxx"
      },
      "source": [
        "###load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMzOLXXrXLN"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"train_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aYySDmfRAF"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ia7Da3rY8a",
        "outputId": "e26aff5a-6d78-4093-f431-3548312ac515"
      },
      "source": [
        "train_gen = datagen(train,train , batch_size)\n",
        "vad_gen = datagen(vad,vad,  batch_size)\n",
        "train_gen.__len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otk59UumZidI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc8d716-0389-4af4-d3cb-051b07d0ef2a"
      },
      "source": [
        "a = train_gen.__getitem__(0)[0][0]\n",
        "b = np.nonzero(a)[0]\n",
        "print(b)\n",
        "\n",
        "print(a[b])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    3    59   194   308   361   546   551   638   860   884  1189  1286\n",
            "  1313  1336  1476  1598  1599  1627  1681  1757  1780  1793  1863  2131\n",
            "  2195  2251  2337  2373  2418  2434  2486  2513  2524  2573  2660  2813\n",
            "  2936  2978  2990  3023  3059  3071  3128  3184  3208  3225  3343  3410\n",
            "  3418  3601  3625  3658  3665  3694  3906  3987  4036  4082  4116  4145\n",
            "  4162  4177  4196  4231  4236  4268  4277  4387  4428  4523  4569  4807\n",
            "  4881  4888  4889  4948  4951  4969  5103  5163  5227  5351  5569  5741\n",
            "  5793  5797  5901  6005  6007  6208  6221  6238  6294  6349  6380  6513\n",
            "  6572  6601  6713  6745  6768  6853  6871  6877  6886  6923  7006  7190\n",
            "  7221  7301  7654  7769  7776  7827  7867  7967  8003  8028  8110  8328\n",
            "  8338  8349  8468  8485  8753  8873  8886  9130  9143  9192  9199  9275\n",
            "  9486  9560  9607  9624  9671  9793  9817  9827  9847  9900  9948  9960\n",
            "  9967  9970  9993 10087 10177]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzvqmDzw4g5_"
      },
      "source": [
        "###Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ugK83oxrQkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bf389d-21d6-47e3-b8e0-59aac59b3331"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "KLBeta = 0\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_primary_train_best.hdf5\"), verbose=1, save_best_only=True)\n",
        "batch_hist = BatchHistory(args.prim_pel, args.prim_pbl, args.prim_pevl) #use primary train loss logs\n",
        "reduce_LR = ReduceLROnPlateau(patience=10,verbose=1, factor=0.5)\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "v.compile(optimizer='adam', loss=VAE_loss)\n",
        "\n",
        "#PICK UP FROM WHERE LAST LEFT OFF\n",
        "complete_epochs = get_num_epochs_complete()\n",
        "remaining_epochs = total_num_epochs - complete_epochs\n",
        "load_model(v, weights)\n",
        "\n",
        "#RUN\n",
        "print(\"commisioned to run for\",total_num_epochs,\"epochs total\")\n",
        "print(\"known loss values found for \",complete_epochs,\"previous epochs\")\n",
        "print(\"will run for\", remaining_epochs, \"more epochs unless stopped early\")\n",
        "\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist, reduce_LR])\n",
        "\n",
        "print(\"complete\")\n",
        "\n",
        "\n",
        "keras.utils.plot_model(v, show_layer_activations=True)\n",
        "v.save_weights(os.path.join(training_results,\"vae_primary_train_last.hdf5\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed to load weights\n",
            "commisioned to run for 30 epochs total\n",
            "known loss values found for  0 previous epochs\n",
            "will run for 30 more epochs unless stopped early\n",
            "Epoch 1/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 1105.1255\n",
            "Epoch 00001: val_loss improved from inf to 1028.36047, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 10s 80ms/step - loss: 1105.1255 - val_loss: 1028.3605 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 1035.8369\n",
            "Epoch 00002: val_loss improved from 1028.36047 to 995.57111, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 9s 90ms/step - loss: 1035.8369 - val_loss: 995.5711 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 972.7130\n",
            "Epoch 00003: val_loss improved from 995.57111 to 979.16101, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 83ms/step - loss: 972.7130 - val_loss: 979.1610 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 970.5654\n",
            "Epoch 00004: val_loss improved from 979.16101 to 957.30481, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 79ms/step - loss: 970.5654 - val_loss: 957.3048 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 922.4066\n",
            "Epoch 00005: val_loss improved from 957.30481 to 910.60919, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 80ms/step - loss: 922.4066 - val_loss: 910.6092 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 924.8052\n",
            "Epoch 00006: val_loss did not improve from 910.60919\n",
            "100/100 [==============================] - 7s 72ms/step - loss: 924.8052 - val_loss: 938.8247 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 859.5314\n",
            "Epoch 00007: val_loss improved from 910.60919 to 847.64978, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 10s 105ms/step - loss: 859.5314 - val_loss: 847.6498 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 823.1550\n",
            "Epoch 00008: val_loss improved from 847.64978 to 838.61914, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 12s 118ms/step - loss: 823.1550 - val_loss: 838.6191 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 812.7514\n",
            "Epoch 00009: val_loss did not improve from 838.61914\n",
            "100/100 [==============================] - 8s 76ms/step - loss: 812.7514 - val_loss: 840.7463 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 801.5294\n",
            "Epoch 00010: val_loss improved from 838.61914 to 823.39368, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 10s 104ms/step - loss: 801.5294 - val_loss: 823.3937 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 791.0951\n",
            "Epoch 00011: val_loss improved from 823.39368 to 811.44122, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 77ms/step - loss: 791.0951 - val_loss: 811.4412 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 796.9608\n",
            "Epoch 00012: val_loss improved from 811.44122 to 797.73639, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 81ms/step - loss: 796.9608 - val_loss: 797.7364 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 794.5214\n",
            "Epoch 00013: val_loss did not improve from 797.73639\n",
            "100/100 [==============================] - 7s 72ms/step - loss: 794.5214 - val_loss: 802.9836 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 785.7625\n",
            "Epoch 00014: val_loss did not improve from 797.73639\n",
            "100/100 [==============================] - 8s 75ms/step - loss: 785.7625 - val_loss: 803.6816 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 776.2143\n",
            "Epoch 00015: val_loss improved from 797.73639 to 788.63080, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 11s 105ms/step - loss: 776.2143 - val_loss: 788.6308 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 773.6036\n",
            "Epoch 00016: val_loss improved from 788.63080 to 784.37811, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_primary_train_best.hdf5\n",
            "100/100 [==============================] - 8s 78ms/step - loss: 773.6036 - val_loss: 784.3781 - lr: 0.0010\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 809.1537\n",
            "Epoch 00017: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 73ms/step - loss: 809.1537 - val_loss: 853.8478 - lr: 0.0010\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 844.9573\n",
            "Epoch 00018: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 73ms/step - loss: 844.9573 - val_loss: 858.5091 - lr: 0.0010\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 807.3322\n",
            "Epoch 00019: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 807.3322 - val_loss: 814.2056 - lr: 0.0010\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 793.9859\n",
            "Epoch 00020: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 793.9859 - val_loss: 845.1965 - lr: 0.0010\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 795.6354\n",
            "Epoch 00021: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 795.6354 - val_loss: 805.6276 - lr: 0.0010\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 788.8100\n",
            "Epoch 00025: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 71ms/step - loss: 788.8100 - val_loss: 797.5932 - lr: 0.0010\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 804.0513\n",
            "Epoch 00026: val_loss did not improve from 784.37811\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 804.0513 - val_loss: 821.4913 - lr: 0.0010\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 801.5498\n",
            "Epoch 00027: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 801.5498 - val_loss: 815.5523 - lr: 5.0000e-04\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 798.6455\n",
            "Epoch 00028: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 798.6455 - val_loss: 813.7148 - lr: 5.0000e-04\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 797.3182\n",
            "Epoch 00029: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 797.3182 - val_loss: 812.4326 - lr: 5.0000e-04\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 796.6852\n",
            "Epoch 00030: val_loss did not improve from 784.37811\n",
            "100/100 [==============================] - 7s 72ms/step - loss: 796.6852 - val_loss: 811.8066 - lr: 5.0000e-04\n",
            "complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFx-DlHv0bNW"
      },
      "source": [
        "##Secondary Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13aCtS771N-M"
      },
      "source": [
        "###load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwk_dMrd1N-T"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_training_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvwe_I2L1N-U"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9XTqDk51N-U",
        "outputId": "aaf648af-f620-47b3-dfb7-1a8509a977cf"
      },
      "source": [
        "train_gen = datagen(train, train , batch_size)\n",
        "vad_gen = datagen(vad , vad, batch_size)\n",
        "train_gen.__len__()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTQKPu11N-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1345d77f-5ffb-4450-b896-e44446fe05c0"
      },
      "source": [
        "a = train_gen.__getitem__(1)[0][0]\n",
        "b = np.nonzero(a)[0]\n",
        "print(b)\n",
        "\n",
        "print(a[b])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  968  1114  1148  1267  1286  1986  2131  2216  2523  2534  2646  2884\n",
            "  2965  3112  3858  4288  4497  5147  5480  5787  5883  5964  6177  6218\n",
            "  6403  6476  6966  7196  7892  8281  8468  8470  9220  9246  9706  9916\n",
            " 10124]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inl2bnFt1N-V"
      },
      "source": [
        "###Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvjfHrmS1N-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c67bb8d-749f-4a43-997e-87afe2ba7802"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "KLBeta = 1\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_secondary_train_best.hdf5\"), verbose=1, save_best_only=True)\n",
        "batch_hist = BatchHistory(args.sec_pel, args.sec_pbl, args.sec_pevl) #use secondary train loss logs\n",
        "reduce_LR = ReduceLROnPlateau(patience=10,verbose=1, factor=0.5)\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "v.compile(optimizer='adam', loss=VAE_loss)\n",
        "\n",
        "remaining_epochs = 10\n",
        "v.load_weights(os.path.join(training_results,\"vae_primary_train_best.hdf5\"))#load best weights from primary\n",
        "\n",
        "#RUN\n",
        "print(\"will run for\", remaining_epochs, \"more epochs unless stopped early\")\n",
        "\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist, reduce_LR])\n",
        "\n",
        "print(\"complete\")\n",
        "\n",
        "v.save_weights(os.path.join(training_results,\"vae_secondary_train_last.hdf5\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will run for 10 more epochs unless stopped early\n",
            "Epoch 1/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 275.7501\n",
            "Epoch 00001: val_loss improved from inf to 2157.65210, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 5s 173ms/step - loss: 275.7501 - val_loss: 2157.6521 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 214.1315\n",
            "Epoch 00002: val_loss improved from 2157.65210 to 2016.72656, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 7s 378ms/step - loss: 214.1315 - val_loss: 2016.7266 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 197.4257\n",
            "Epoch 00003: val_loss improved from 2016.72656 to 1939.99475, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 3s 157ms/step - loss: 197.4257 - val_loss: 1939.9948 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 191.3111\n",
            "Epoch 00004: val_loss improved from 1939.99475 to 1896.81665, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 4s 208ms/step - loss: 191.3111 - val_loss: 1896.8167 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 195.9933\n",
            "Epoch 00005: val_loss improved from 1896.81665 to 1891.71497, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 4s 192ms/step - loss: 195.9933 - val_loss: 1891.7150 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 190.3420\n",
            "Epoch 00006: val_loss improved from 1891.71497 to 1836.08362, saving model to /content/drive/MyDrive/COMP700_Honours Project/vae_ml10m_training_results/vae_secondary_train_best.hdf5\n",
            "20/20 [==============================] - 4s 210ms/step - loss: 190.3420 - val_loss: 1836.0836 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 189.5835\n",
            "Epoch 00007: val_loss did not improve from 1836.08362\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 189.5835 - val_loss: 1845.7080 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 195.5121\n",
            "Epoch 00008: val_loss did not improve from 1836.08362\n",
            "20/20 [==============================] - 2s 116ms/step - loss: 195.5121 - val_loss: 1913.5242 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 188.3505\n",
            "Epoch 00009: val_loss did not improve from 1836.08362\n",
            "20/20 [==============================] - 3s 131ms/step - loss: 188.3505 - val_loss: 1870.8152 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 187.0478\n",
            "Epoch 00010: val_loss did not improve from 1836.08362\n",
            "20/20 [==============================] - 2s 119ms/step - loss: 187.0478 - val_loss: 1857.9082 - lr: 0.0010\n",
            "complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxZMmBctKkUC"
      },
      "source": [
        "#Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK-jIiU_aX-t"
      },
      "source": [
        "##Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "BOw3gzaFoWhI",
        "outputId": "e174375a-bec3-49b6-e654-783a11562607"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "training_results = \"vae_y_training_results\"\n",
        "max_y = 100\n",
        "max_x , step = 11 , 1\n",
        "relevant_loss_data = args.sec_pel\n",
        "\n",
        "loss_file = os.path.join(args.root, training_results, relevant_loss_data)\n",
        "with open (loss_file, \"r\") as f:\n",
        "  #a = f.readlines()\n",
        "  a = np.array([i.strip() for i in f.readlines()][1:]).astype(np.float)\n",
        "  a = [float(i) for i in a]\n",
        "\n",
        "print(len(a))\n",
        "l = [np.inf]\n",
        "l.extend(a)\n",
        "f = plt.plot(l)\n",
        "plt.ylim(0, max_y)\n",
        "\n",
        "plt.xticks(np.arange(0, max_x, step))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f92ef1234d0>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef1235d0>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef0db850>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef07aed0>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef083450>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef083990>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef083f10>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef083fd0>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef08a490>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef08a910>,\n",
              "  <matplotlib.axis.XTick at 0x7f92ef08ae10>],\n",
              " <a list of 11 Text major ticklabel objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gohX3m-aao1"
      },
      "source": [
        "##Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMBz6U4etcCA"
      },
      "source": [
        "test = pickle.load( \n",
        "    open(os.path.join(input_data,\"train_impl.pkl\") , 'rb')\n",
        ")\n",
        "\n",
        "test_gen = datagen(test,test , 1)#args.batchsize)\n",
        "\n",
        "original_dim = test.shape[1]\n",
        "\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "v.compile(optimizer='adam', loss=VAE_loss)\n",
        "\n",
        "weights = os.path.join(training_results, \"vae_secondary_train_best.hdf5\")\n",
        "v.load_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQce7H18bBmX",
        "outputId": "76c23f41-2fe0-4587-b0d6-dfa4f28b6717"
      },
      "source": [
        "#if movielens:\n",
        "# max = 49877 (train) and max = 9999 (validation, test-train or test-test)\n",
        "\n",
        "#if yelp:\n",
        "# max = 345663 (train) and max = 9999 (validation, test-train or test-test)\n",
        "\n",
        "u_num = 2\n",
        "\n",
        "ins = np.array([test_gen.__getitem__(u_num)[0][0]])\n",
        "a = v.predict(ins)\n",
        "\n",
        "a = -np.sort(-a)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35983902, 0.31730312, 0.3064789 , ..., 0.        , 0.        ,\n",
              "        0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hJ037Rc6PEa"
      },
      "source": [
        "*MOVIELENS*: \n",
        "\n",
        "After filtering, there are 9998816 events from 69878 users and 10196 businesses (sparsity: 1.403%)\n",
        "\n",
        "sample prediction: [[0.43879703, 0.4058009 , 0.4028352 , ..., 0.        , 0.        ,\n",
        "        0.        ]]\n",
        "\n",
        "\n",
        "*YELP*:\n",
        "\n",
        "(b only filter) After filtering, there are 8635403 events from 2189457 users and 160585 businesses (sparsity: 0.002%)\n",
        "\n",
        "(u only filter) After filtering, there are 5766970 events from 365665 users and 159108 businesses (sparsity: 0.010%)\n",
        "\n",
        "(full filter) After filtering, there are 5674527 events from 365664 users and 127351 businesses (sparsity: 0.012%)\n",
        "\n",
        "sample prediction: [[0., 0., 0., ..., 0., 0., 0.]]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rMjapfG449"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8rn6AEzLTU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a45b533-59a9-4aed-d2eb-73ac482f31b2"
      },
      "source": [
        "!pip3 install tensorflow-ranking\n",
        "\n",
        "import tensorflow_ranking as tfr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-ranking\n",
            "  Downloading tensorflow_ranking-0.5.0-py2.py3-none-any.whl (141 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▎                             | 10 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 20 kB 32.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 30 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 40 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 51 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 61 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 71 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 81 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 92 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 102 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 112 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 122 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 133 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 141 kB 12.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.19.5)\n",
            "Collecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
            "  Downloading tensorflow_serving_api-2.7.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tensorflow<3,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.41.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.1.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (12.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.13.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.10.0.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.3.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.22.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.3.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<3,>=2.7.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.6.0)\n",
            "Installing collected packages: tensorflow-serving-api, tensorflow-ranking\n",
            "Successfully installed tensorflow-ranking-0.5.0 tensorflow-serving-api-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r83NUj7HqeS"
      },
      "source": [
        "test_true = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_testing_impl.pkl\") , 'rb')\n",
        ")\n",
        "\n",
        "test_given = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_testing_impl.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwqIdmTHqeY"
      },
      "source": [
        "total_num_samples = test_given.shape[0]\n",
        "original_dim = test_given.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vK4GPHbHqeZ",
        "outputId": "7fa536c8-1df0-4e52-d30e-d07433fbb63e"
      },
      "source": [
        "test_gen = datagen(test_given ,test_true, args.batchsize)\n",
        "\n",
        "test_gen.__getitem__(0)[0].shape#____len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 127351)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRKYSf8-H7M7"
      },
      "source": [
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "KLBeta = 1\n",
        "metrics = [ tfr.keras.metrics.RecallMetric(name = \"recall_20\", topn=20,),\n",
        "           tfr.keras.metrics.RecallMetric(name = \"recall_50\", topn=50,),\n",
        "           tfr.keras.metrics.NDCGMetric(name = \"ndcg_100\", topn=100,)         \n",
        "]\n",
        "\n",
        "v.compile(optimizer='adam', loss=VAE_loss, metrics = metrics)\n",
        "v.load_weights(os.path.join(training_results,\"vae_secondary_train_best.hdf5\"))#load best weights from secondary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Dn-xWNIO3r",
        "outputId": "08bbfe25-ed31-4353-fd2f-4e9429febf29"
      },
      "source": [
        "preds = v.evaluate(test_gen, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 584s 29s/step - loss: 270.4864 - recall_20: 1.5548e-04 - recall_50: 3.7654e-04 - ndcg_100: 3.3082e-04\n"
          ]
        }
      ]
    }
  ]
}