{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaelinN/VAE_RECOMMENDER_IMPLICIT_FEEDBACK/blob/main/VAE_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFjVqXpgQ-E"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rNgvglTWCN"
      },
      "source": [
        "#!pip3 install tensorflow-ranking\n",
        "#!pip3 install tensorflow\n",
        "#!pip3 install numpy\n",
        "#!pip3 install pickle\n",
        "#!pip3 install math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as tfback\n",
        "from tensorflow.keras import layers, activations, Model, losses\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "#import tensorflow_ranking as tfr\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pRl4vBS7LM"
      },
      "source": [
        "class argclass(object):\n",
        "  def __init__(self):\n",
        "    self.root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "    self.input_data = \"Data/remove_low_interaction_users/split/matrices/implicit\"\n",
        "    self.training_results =\"training_results\"\n",
        "\n",
        "    self.prim_pel = \"prim_pel.txt\"\n",
        "    self.prim_pbl = \"prim_pbl.txt\"\n",
        "    self.prim_pevl = \"prim_pevl.txt\"\n",
        "\n",
        "    self.sec_pel = \"sec_pel.txt\"\n",
        "    self.sec_pbl = \"sec_pbl.txt\"\n",
        "    self.sec_pevl = \"sec_pevl.txt\"\n",
        "\n",
        "    self.weights = None #\"vae_epoch_01_loss_210.97.hdf5\"\n",
        "    self.intermediatedim = 512 \n",
        "    self.epochs = 10\n",
        "    self.batchsize = 200\n",
        "    self.klannealrate = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aj-nLb6kRj5",
        "outputId": "80a33824-141f-4dc1-ee96-d742b3582224"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtzkG8xR_zO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "69e626a4-3b9a-41ad-fd0b-080675fec0be"
      },
      "source": [
        "'''\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input_data', type=str)\n",
        "parser.add_argument('--training_results', type=str, default=\"training_results\")\n",
        "\n",
        "parser.add_argument('--prim_pel', type=str, default=\"prim_pel.txt\")\n",
        "parser.add_argument('--prim_pbl', type=str, default=\"prim_pbl.txt\")\n",
        "parser.add_argument('--prim_pevl', type=str, default=\"prim_pevl.txt\")\n",
        "\n",
        "parser.add_argument('--sec_pel', type=str, default=\"sec_pel.txt\")\n",
        "parser.add_argument('--sec_pbl', type=str, default=\"sec_pbl.txt\")\n",
        "parser.add_argument('--sec_pevl', type=str, default=\"sec_pevl.txt\")\n",
        "\n",
        "\n",
        "parser.add_argument('--weights', type=str)\n",
        "parser.add_argument('--intermediatedim', type=int)\n",
        "parser.add_argument('--epochs', type=int)\n",
        "parser.add_argument('--batchsize', type=int)\n",
        "parser.add_argument('--klannealrate', type=float)\n",
        "args = parser.parse_args()\n",
        "'''\n",
        "\n",
        "\n",
        "args = argclass()\n",
        "root = args.root\n",
        "\n",
        "input_data = args.input_data\n",
        "input_data = os.path.join(root, input_data)\n",
        "\n",
        "training_results = args.training_results\n",
        "training_results = os.path.join(root, training_results)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_results)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "#IF NO WEIGHTS FILE TO USE, THEN JUST LEAVE ARG OUT. THE LAOD FUNCTION WILL KNOW TO START ANEW\n",
        "# Checkpoint callback saves into filepath=os.path.join(training_results,\"vae_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\")\n",
        "weights = args.weights\n",
        "try:\n",
        "  weights = os.path.join(training_results, weights)\n",
        "except TypeError:\n",
        "  pass\n",
        "\n",
        "#annealing_rate = args.klannealrate if not args.klannealrate is None else 0.0001\n",
        "#intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "#total_num_epochs = args.epochs if args.epochs is not None else 5\n",
        "#batch_size = args.batchsize if args.batchsize is not None else 400\n",
        "\n",
        "\n",
        "\n",
        "print(root)\n",
        "print(input_data)\n",
        "print(training_results)\n",
        "print(weights)\n",
        "\"\"\"\n",
        "print(annealing_rate)\n",
        "print(intermediate_dim)\n",
        "print(total_num_epochs)\n",
        "print(batch_size)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COMP700_Honours Project\n",
            "/content/drive/MyDrive/COMP700_Honours Project/Data/remove_low_interaction_users/matrices/implicit\n",
            "/content/drive/MyDrive/COMP700_Honours Project/models\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nprint(annealing_rate)\\nprint(intermediate_dim)\\nprint(total_num_epochs)\\nprint(batch_size)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8A_Q0_gVfR"
      },
      "source": [
        "#Model design\n",
        "* sample layer\n",
        "* batch history callback\n",
        "* vae loss function\n",
        "* VAE builder class\n",
        "* data generator sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5rKmGkUo_V"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, name=\"Sampling\", **kwargs):\n",
        "    super(Sampling, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "\n",
        "\n",
        "    #epsilon = distribution.sample()\n",
        "    \n",
        "    epsilon = tfback.random_normal(shape=(batch,dim))\n",
        "    sample = epsilon * tf.exp(0.5 * z_log_var)  +   z_mean  #Reparametrization trick: convert from standard normal to desired distribution\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvcmpfPd3Sg"
      },
      "source": [
        "class BatchHistory(keras.callbacks.Callback):  \n",
        "  def __init__(self, pel, pbl, pevl):\n",
        "    super(BatchHistory,self).__init__() \n",
        "    self.loss = [] \n",
        "    self.val_loss = 0.0\n",
        "    self.pel = pel\n",
        "    self.pbl = pbl\n",
        "    self.pevl = pevl\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):   \n",
        "    self.loss.append(logs.get('loss'))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.val_loss = logs.get('val_loss')\n",
        "    self.loss = [str(i) for i in self.loss]\n",
        "\n",
        "    with open(os.path.join(training_results,pevl), \"a\") as pe_v_loss:\n",
        "      pe_v_loss.write('\\n'+ str(self.val_loss))\n",
        "\n",
        "    with open(os.path.join(training_results,pbl), \"a\") as pb_loss:\n",
        "      pb_loss.write(\"\\n\")\n",
        "      pb_loss.writelines('\\n'.join(self.loss))\n",
        "\n",
        "    with open(os.path.join(training_results,pel), \"a\") as pe_loss:\n",
        "      pe_loss.write('\\n'+ str(self.loss[len(self.loss)-1]))\n",
        "    self.loss = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mHDOrkYFlK"
      },
      "source": [
        "#original_dim = 127350\n",
        "annealing_rate = args.klannealrate if not args.klannealrate is None else 0.0001\n",
        "KLBeta = 0\n",
        "def VAE_loss(y_true, y_pred):\n",
        "    global KLBeta\n",
        "    reconst_loss =  original_dim * losses.binary_crossentropy(y_true, y_pred)\n",
        "    KLDiv = KLBeta * losses.kl_divergence(y_true, y_pred)\n",
        "\n",
        "    KLBeta = min(KLBeta+annealing_rate , 1) #update weight of KL factor\n",
        "\n",
        "    #return (KLDiv,reconst_loss)\n",
        "\n",
        "    return  reconst_loss + KLDiv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_ca8PXgjYB"
      },
      "source": [
        "class vae_builder(object):\n",
        "  def __init__(self, original_dim, intermediate_dim, latent_dim, name='VAE'):\n",
        "    self.name = name\n",
        "    self.original_dim = original_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  \n",
        "  def build(self):\n",
        "    self.input = layers.Input(self.original_dim, name = 'input')\n",
        "    self.dropout = layers.Dropout(rate=0.5)(self.input)\n",
        "\n",
        "    #encoder\n",
        "    self.e1 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e1')(self.dropout)\n",
        "    self.e2 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e2')(self.e1)\n",
        "    self.e3 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e3')(self.e2)\n",
        "    self.e4 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e4')(self.e3)\n",
        "    self.e5 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e5')(self.e4)\n",
        "\n",
        "    #sampling\n",
        "    self.mean = layers.Dense(self.latent_dim, name = 'mean')(self.e5)  \n",
        "    self.log_var = layers.Dense(self.latent_dim, name = 'log_var')(self.e5)  \n",
        "    self.sampling = Sampling()([self.mean, self.log_var])\n",
        "\n",
        "    #decoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='relu',name = 'd1')(self.sampling)\n",
        "    self.d2 = layers.Dense(self.original_dim, activation='relu', name = 'd2')(self.d1)  \n",
        "    \n",
        "    self.output = layers.Activation(activations.swish, name = 'output')(self.d2)\n",
        "\n",
        "    #to model\n",
        "    vae = Model(inputs = self.input, outputs = self.output, name = self.name)\n",
        "    return vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjfLEDZeTr2"
      },
      "source": [
        "class datagen(keras.utils.Sequence):\n",
        "  def __init__(self, x_set, batch_size, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "  def __len__(self):\n",
        "      #return math.ceil((self.x.shape[0]) / self.batch_size)\n",
        "      return len(self.shuffled_idx)\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "\n",
        "    batch_y = np.array(self.x[idx].todense())\n",
        "\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNmS-DzMgZRC"
      },
      "source": [
        "#Primary Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZbIuw34Dxx"
      },
      "source": [
        "##load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMzOLXXrXLN"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_data,\"train_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aYySDmfRAF"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]\n",
        "intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "\n",
        "total_num_epochs = args.epochs if args.epochs is not None else 5\n",
        "\n",
        "batch_size = args.batchsize if args.batchsize is not None else 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ia7Da3rY8a"
      },
      "source": [
        "train_gen = datagen(train , batch_size)\n",
        "vad_gen = datagen(vad ,  batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-AG8m04TBI"
      },
      "source": [
        "##code to allow continuation of training over many sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mlp-VtwfeyE"
      },
      "source": [
        "def load_model(v, weights_file):\n",
        "  try:\n",
        "    v.load_weights(weights_file)\n",
        "    print(\"weights loaded successfully\")\n",
        "  except:\n",
        "    print(\"failed to load weights\")\n",
        "\n",
        "def get_num_epochs_complete():\n",
        "  try:\n",
        "    with open(os.path.join(training_results,args.prim_pevl), \"r\") as f:\n",
        "      lines = [i for i in f.readlines() if len(i.strip()) > 0]\n",
        "      return len(lines)\n",
        "  except FileNotFoundError:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzvqmDzw4g5_"
      },
      "source": [
        "##Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ugK83oxrQkA"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_primary_train_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\"), verbose=1, save_best_only=False)\n",
        "batch_hist = BatchHistory(args.prim_pel, args.prim_pbl, args.prim_pevl)\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "#configurable optimiser\n",
        "slow_adam = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "\n",
        "v.compile(optimizer=slow_adam, loss=VAE_loss)\n",
        "\n",
        "#PICK UP FROM WHERE LEFT OFF\n",
        "complete_epochs = get_num_epochs_complete()\n",
        "remaining_epochs = total_num_epochs - complete_epochs\n",
        "load_model(v, weights)\n",
        "\n",
        "#RUN\n",
        "print(\"commisioned to run for\",total_num_epochs,\"epochs total\")\n",
        "print(\"known loss values found for \",complete_epochs,\"previous epochs\")\n",
        "print(\"will run for\", remaining_epochs, \"more epochs unless stopped early\")\n",
        "\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist])\n",
        "\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkozKDwuERB2"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiHgRBoQSPeB"
      },
      "source": [
        "#REMINDER:\n",
        "* Train on train to replicaate train (with fuzziness i.e. new recommendations)\n",
        "\n",
        "* train on test_train to replicate test_train (with fuzziness i.e. new recommendations i.e hopefully something that has stuff in common with test_test)\n",
        "\n",
        "* test on test_train to see if the fuzziness is actually what is in test_test\n",
        "\n",
        "* eval on test_test to compare the fuzziness around output of test_train to the true vals of test_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZvGDwjWETvf"
      },
      "source": [
        "##Secondary training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CvBbZ5XETJj"
      },
      "source": [
        "test_train = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_training_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")\n",
        "\n",
        "total_num_samples = test_train.shape[0]\n",
        "original_dim = test_train.shape[1]\n",
        "intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "\n",
        "total_num_epochs = args.epochs if args.epochs is not None else 5\n",
        "\n",
        "batch_size = args.batchsize if args.batchsize is not None else 400\n",
        "\n",
        "\n",
        "test_train_gen = datagen(test_train , batch_size)\n",
        "vad_gen = datagen(vad ,  batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "2LTaxRJUI4Lr",
        "outputId": "ab908301-d1aa-425c-c15d-322e52fd7f8d"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_final_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\"), verbose=1, save_best_only=False)\n",
        "batch_hist = BatchHistory(args.sec_pel, args.sec_pbl, args.sec_pevl)\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "#configurable optimiser\n",
        "slow_adam = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "'''\n",
        "metrics = [\n",
        "           tfr.keras.metrics.NDCGMetric(topn=20),\n",
        "           tfr.keras.metrics.RecallMetric(topn=20)\n",
        "           ]\n",
        "'''\n",
        "v.compile(optimizer=slow_adam, loss=VAE_loss)#, metrics = metrics)\n",
        "\n",
        "#PICK UP FROM WHERE LEFT OFF\n",
        "remaining_epochs = total_num_epochs # no start from crash since only small amount of data in secondary train\n",
        "load_model(v, weights)\n",
        "\n",
        "#RUN\n",
        "print(\"commisioned to run for\",total_num_epochs,\"epochs total\")\n",
        "print(\"will run for\", remaining_epochs, \"more epochs\")\n",
        "\n",
        "v.fit(test_train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist])\n",
        "\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "failed to load weights\n",
            "commisioned to run for 10 epochs total\n",
            "will run for 10 more epochs\n",
            "Epoch 1/10\n",
            "   93/10000 [..............................] - ETA: 1:01:42 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-79fe6de80914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"will run for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"more epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_train_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvad_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_hist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWE-osS1z7IT"
      },
      "source": [
        "#keras.utils.plot_model(v, show_shapes=True, to_file=os.path.join(root, \"vae.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
