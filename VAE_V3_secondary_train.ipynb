{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_V3_secondary_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GZ8A_Q0_gVfR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaelinN/VAE_RECOMMENDER_IMPLICIT_FEEDBACK/blob/main/VAE_V3_secondary_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFjVqXpgQ-E"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rNgvglTWCN"
      },
      "source": [
        "#!pip3 install tensorflow-ranking\n",
        "#!pip3 install tensorflow\n",
        "#!pip3 install numpy\n",
        "#!pip3 install pickle\n",
        "#!pip3 install math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as tfback\n",
        "from tensorflow.keras import layers, activations, Model, losses\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "#import tensorflow_ranking as tfr\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import math\n",
        "import argparse"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aj-nLb6kRj5",
        "outputId": "80a33824-141f-4dc1-ee96-d742b3582224"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcx6JQdM5Q52"
      },
      "source": [
        "\"\"\"\n",
        "#uncomment if running as .py on cluster such as HIPPO\n",
        "#comment if running as .ipynb on Google Colab\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input_data', type=str)\n",
        "parser.add_argument('--training_results', type=str, default=\"training_results\")\n",
        "\n",
        "parser.add_argument('--prim_pel', type=str, default=\"prim_pel.txt\")\n",
        "parser.add_argument('--prim_pbl', type=str, default=\"prim_pbl.txt\")\n",
        "parser.add_argument('--prim_pevl', type=str, default=\"prim_pevl.txt\")\n",
        "\n",
        "parser.add_argument('--sec_pel', type=str, default=\"sec_pel.txt\")\n",
        "parser.add_argument('--sec_pbl', type=str, default=\"sec_pbl.txt\")\n",
        "parser.add_argument('--sec_pevl', type=str, default=\"sec_pevl.txt\")\n",
        "\n",
        "\n",
        "parser.add_argument('--weights', type=str)\n",
        "parser.add_argument('--intermediatedim', type=int)\n",
        "parser.add_argument('--epochs', type=int)\n",
        "parser.add_argument('--batchsize', type=int)\n",
        "parser.add_argument('--klannealrate', type=float)\n",
        "args = parser.parse_args()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pRl4vBS7LM"
      },
      "source": [
        "#uncomment if running as .ipynb on Google Colabas\n",
        "#comment if running as  .py on cluster such as HIPPO\n",
        "class argclass(object):\n",
        "  def __init__(self):\n",
        "    self.root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "    self.input_data = \"Data/remove_low_interaction_users/split/matrices/implicit\"\n",
        "    self.training_results =\"training_results\"\n",
        "\n",
        "    self.prim_pel = \"prim_pel.txt\"\n",
        "    self.prim_pbl = \"prim_pbl.txt\"\n",
        "    self.prim_pevl = \"prim_pevl.txt\"\n",
        "\n",
        "    self.sec_pel = \"sec_pel.txt\"\n",
        "    self.sec_pbl = \"sec_pbl.txt\"\n",
        "    self.sec_pevl = \"sec_pevl.txt\"\n",
        "\n",
        "    self.weights = None #\"vae_epoch_01_loss_210.97.hdf5\"\n",
        "    self.intermediatedim = 512 \n",
        "    self.epochs = 10\n",
        "    self.batchsize = 200\n",
        "    self.klannealrate = 0.001\n",
        "args = argclass()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtzkG8xR_zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f51e34-d049-4bc3-cfcf-7af6f1b5be0c"
      },
      "source": [
        "root = args.root\n",
        "\n",
        "input_data = os.path.join(root, args.input_data)\n",
        "\n",
        "training_results = os.path.join(root, args.training_results)\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_results)\n",
        "exceptFileExistsError:\n",
        "  pass\n",
        "\n",
        "weights = args.weights\n",
        "if weights is None:\n",
        "  sys.stderr.write(\"Weights cannot be None. Perform primary training and load a resultant 'filename.hdf5' weights file as an arg\")\n",
        "  exit(-1)\n",
        "weights = os.path.join(training_results, weights)\n",
        "\"\"\"\n",
        "try:\n",
        "  weights = os.path.join(training_results, weights)\n",
        "except TypeError:\n",
        "  pass\n",
        "\"\"\"\n",
        "\n",
        "#annealing_rate = args.klannealrate if not args.klannealrate is None else 0.0001\n",
        "#intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "#total_num_epochs = args.epochs if args.epochs is not None else 5\n",
        "#batch_size = args.batchsize if args.batchsize is not None else 400\n",
        "\n",
        "\n",
        "\n",
        "print(root)\n",
        "print(input_data)\n",
        "print(training_results)\n",
        "print(weights)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COMP700_Honours Project\n",
            "/content/drive/MyDrive/COMP700_Honours Project/Data/remove_low_interaction_users/split/matrices/implicit\n",
            "/content/drive/MyDrive/COMP700_Honours Project/training_results\n",
            "/content/drive/MyDrive/COMP700_Honours Project/training_results/lie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8A_Q0_gVfR"
      },
      "source": [
        "#Model design\n",
        "* same as primary\n",
        "(minor change: higher annealing rate)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5rKmGkUo_V"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, name=\"Sampling\", **kwargs):\n",
        "    super(Sampling, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "\n",
        "\n",
        "    #epsilon = distribution.sample()\n",
        "    \n",
        "    epsilon = tfback.random_normal(shape=(batch,dim))\n",
        "    sample = epsilon * tf.exp(0.5 * z_log_var)  +   z_mean  #Reparametrization trick: convert from standard normal to desired distribution\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvcmpfPd3Sg"
      },
      "source": [
        "class BatchHistory(keras.callbacks.Callback):  \n",
        "  def __init__(self, pel, pbl, pevl):\n",
        "    super(BatchHistory,self).__init__() \n",
        "    self.loss = [] \n",
        "    self.val_loss = 0.0\n",
        "    self.pel = pel\n",
        "    self.pbl = pbl\n",
        "    self.pevl = pevl\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):   \n",
        "    self.loss.append(logs.get('loss'))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.val_loss = logs.get('val_loss')\n",
        "    self.loss = [str(i) for i in self.loss]\n",
        "\n",
        "    with open(os.path.join(training_results,pevl), \"a\") as pe_v_loss:\n",
        "      pe_v_loss.write('\\n'+ str(self.val_loss))\n",
        "\n",
        "    with open(os.path.join(training_results,pbl), \"a\") as pb_loss:\n",
        "      pb_loss.write(\"\\n\")\n",
        "      pb_loss.writelines('\\n'.join(self.loss))\n",
        "\n",
        "    with open(os.path.join(training_results,pel), \"a\") as pe_loss:\n",
        "      pe_loss.write('\\n'+ str(self.loss[len(self.loss)-1]))\n",
        "    self.loss = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mHDOrkYFlK"
      },
      "source": [
        "#original_dim = 127350\n",
        "annealing_rate = args.klannealrate if not args.klannealrate is None else 0.001 #10x faster annealing in expectation of less training data. should it be so? is annealing even required in this phase?\n",
        "KLBeta = 0\n",
        "def VAE_loss(y_true, y_pred):\n",
        "    global KLBeta\n",
        "    reconst_loss =  original_dim * losses.binary_crossentropy(y_true, y_pred)\n",
        "    KLDiv = KLBeta * losses.kl_divergence(y_true, y_pred)\n",
        "\n",
        "    KLBeta = min(KLBeta+annealing_rate , 1) #update weight of KL factor\n",
        "\n",
        "    #return (KLDiv,reconst_loss)\n",
        "\n",
        "    return  reconst_loss + KLDiv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_ca8PXgjYB"
      },
      "source": [
        "class vae_builder(object):\n",
        "  def __init__(self, original_dim, intermediate_dim, latent_dim, name='VAE'):\n",
        "    self.name = name\n",
        "    self.original_dim = original_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  \n",
        "  def build(self):\n",
        "    self.input = layers.Input(self.original_dim, name = 'input')\n",
        "    self.dropout = layers.Dropout(rate=0.5)(self.input)\n",
        "\n",
        "    #encoder\n",
        "    self.e1 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e1')(self.dropout)\n",
        "    self.e2 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e2')(self.e1)\n",
        "    self.e3 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e3')(self.e2)\n",
        "    self.e4 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e4')(self.e3)\n",
        "    self.e5 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e5')(self.e4)\n",
        "\n",
        "    #sampling\n",
        "    self.mean = layers.Dense(self.latent_dim, name = 'mean')(self.e5)  \n",
        "    self.log_var = layers.Dense(self.latent_dim, name = 'log_var')(self.e5)  \n",
        "    self.sampling = Sampling()([self.mean, self.log_var])\n",
        "\n",
        "    #decoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='relu',name = 'd1')(self.sampling)\n",
        "    self.d2 = layers.Dense(self.original_dim, activation='relu', name = 'd2')(self.d1)  \n",
        "    \n",
        "    self.output = layers.Activation(activations.swish, name = 'output')(self.d2)\n",
        "\n",
        "    #to model\n",
        "    vae = Model(inputs = self.input, outputs = self.output, name = self.name)\n",
        "    return vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjfLEDZeTr2"
      },
      "source": [
        "class datagen(keras.utils.Sequence):\n",
        "  def __init__(self, x_set, batch_size, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "  def __len__(self):\n",
        "      #return math.ceil((self.x.shape[0]) / self.batch_size)\n",
        "      return len(self.shuffled_idx)\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "\n",
        "    batch_y = np.array(self.x[idx].todense())\n",
        "\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiHgRBoQSPeB"
      },
      "source": [
        "#Secondary Training\n",
        "##reminder:\n",
        "* Train on train to replicaate train (with fuzziness i.e. new recommendations)\n",
        "\n",
        "* train on test_train to replicate test_train (with fuzziness i.e. new recommendations i.e hopefully something that has stuff in common with test_test)\n",
        "\n",
        "* test on test_train to see if the fuzziness is actually what is in test_test\n",
        "\n",
        "* eval on test_test to compare the fuzziness around output of test_train to the true vals of test_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CvBbZ5XETJj"
      },
      "source": [
        "test_train = pickle.load( \n",
        "    open(os.path.join(input_data,\"test_training_impl.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_data,\"vad_impl.pkl\") , 'rb')\n",
        ")\n",
        "\n",
        "total_num_samples = test_train.shape[0]\n",
        "original_dim = test_train.shape[1]\n",
        "intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "total_num_epochs = 2 #args.epochs if args.epochs is not None else 5\n",
        "batch_size = args.batchsize if args.batchsize is not None else 400\n",
        "\n",
        "\n",
        "test_train_gen = datagen(test_train , batch_size)\n",
        "vad_gen = datagen(vad ,  batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LTaxRJUI4Lr"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(training_results,\"vae_final_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\"), verbose=1, save_best_only=False)\n",
        "batch_hist = BatchHistory(args.sec_pel, args.sec_pbl, args.sec_pevl)\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "#configurable optimiser\n",
        "slow_adam = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "'''\n",
        "metrics = [\n",
        "           tfr.keras.metrics.NDCGMetric(topn=20),\n",
        "           tfr.keras.metrics.RecallMetric(topn=20)\n",
        "           ]\n",
        "'''\n",
        "v.compile(optimizer=slow_adam, loss=VAE_loss)#, metrics = metrics)\n",
        "\n",
        "#PICK UP FROM WHERE LEFT OFF\n",
        "remaining_epochs = total_num_epochs # no start from crash since only small amount of data in secondary train\n",
        "v.load_weights(weights)\n",
        "\n",
        "#RUN\n",
        "print(\"commisioned to run for\",total_num_epochs,\"epochs total\")\n",
        "print(\"will run for\", remaining_epochs, \"more epochs\")\n",
        "\n",
        "v.fit(test_train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist])\n",
        "\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWE-osS1z7IT"
      },
      "source": [
        "#keras.utils.plot_model(v, show_shapes=True, to_file=os.path.join(root, \"vae.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}