{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVD++.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "miTGLX9W-oFw",
        "9tLHoyPd0_SV",
        "VmucIS9O-0x-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsN6RC0pTkZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e70bab-5935-4c05-d23e-afd03944415b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbz2xac3-guS"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxPLB_qFpTo8"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEWhEBJJrqWa"
      },
      "source": [
        "!pip install surprise\n",
        "import surprise\n",
        "from surprise.trainset import Trainset\n",
        "from  surprise.dataset import Dataset, DatasetAutoFolds\n",
        "from surprise.reader import Reader\n",
        "from surprise.prediction_algorithms.matrix_factorization import SVDpp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R341I6rvQhOF"
      },
      "source": [
        "\"\"\"\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input_data', type=str)\n",
        "parser.add_argument('--training_results', type=str, default=\"svd_ml10m_training_results\")\n",
        "parser.add_argument('--eval_results', type=str, default= \"eval_results\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\"\"\"\n",
        "\n",
        "#uncomment if running as .ipynb on Google Colabas\n",
        "#comment if running as  .py on cluster such as HIPPO\n",
        "class argclass(object):\n",
        "  def __init__(self):\n",
        "    self.root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "    self.input_data = \"Data/movielens_10m/\"\n",
        "    self.training_results =\"svd_ml10m_training_results\"\n",
        "    self.eval_results = \"eval_results\"\n",
        "args = argclass()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vtlUOfoh6sE"
      },
      "source": [
        "root = args.root\n",
        "data = os.path.join(root,args.input_data)\n",
        "training_results = os.path.join(root, args.training_results)\n",
        "eval_results = os.path.join(training_results, args.eval_results)\n",
        "\n",
        "try:\n",
        "  os.mkdir(training_results)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  os.mkdir(eval_results)\n",
        "except FileExistsError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tLHoyPd0_SV"
      },
      "source": [
        "#TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cze6TwpJ1OhP"
      },
      "source": [
        "reader = Reader(rating_scale=(0,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shl_-6p6iBdm",
        "outputId": "5659f72c-542b-4265-b312-c2702a91fb14"
      },
      "source": [
        "train_data = pd.read_csv(os.path.join(data,\"split/train_rec.csv\")) #PRIMARY TRAINING DATA\n",
        "\n",
        "vad_data = pd.read_csv(os.path.join(data,\"split/vad_rec.csv\"))  #OPTIONAL WHETHER TO ADD OR NOT. WHICH IS MORE FAIR TO THE EXPERIMENT? (LEAVE OUT OR INCLUDE VAD FOR TRAINING FOR SVD++?)\n",
        "test_train_data = pd.read_csv(os.path.join(data,\"split/test_training_rec.csv\")) #HAS TO BE ADDED IN SO SVD++ HAS SEEN THE USERS INVOLVED\n",
        "\n",
        "\n",
        "print(train_data.shape)\n",
        "train_data = pd.concat([train_data, vad_data, test_train_data])\n",
        "\n",
        "\n",
        "#NOTE: SVDpp using explicit form since it has its own internal mechanism to get implicit data\n",
        "\n",
        "\n",
        "for i in range(len(test_train_data['rate'])):\n",
        "  test_train_data['rate'][i] = 1\n",
        "\n",
        "print(train_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7133394, 3)\n",
            "(9718293, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8tb2O87u6Z_"
      },
      "source": [
        "\n",
        "train_datasetautofolds = DatasetAutoFolds(df = train_data[['uid','bid','rate']], reader=reader)\n",
        "trainset = train_datasetautofolds.build_full_trainset()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S86LIjSGzP2Z"
      },
      "source": [
        "model = SVDpp(verbose=True,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6tvBuTz1DiM"
      },
      "source": [
        "model.fit(trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS7t4xi_r87d"
      },
      "source": [
        "pickle.dump(model, open(os.path.join(training_results,\"svdpp_model.pkl\",) , \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmucIS9O-0x-"
      },
      "source": [
        "#Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxc8wQhtPY8h"
      },
      "source": [
        "model = pickle.load(open(os.path.join(training_results,\"svdpp_model.pkl\",) , \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRRusJP4COzP"
      },
      "source": [
        "def predict_with_svd(svd_model, input_recs, u_lookup, b_lookup, prev_cutoff=0):\n",
        "  \n",
        "  temp_dir_pred = \"temp_preds_svd.csv\"\n",
        "  #temp_dir_pred_svd = os.path.join(root, temp_dir_pred_svd)\n",
        "  \n",
        "  columns = ['uid','bid','rate']\n",
        "  results = []\n",
        "  \n",
        "  for i in range(prev_cutoff, len(input_recs['uid'])):\n",
        "\n",
        "    uid = input_recs['uid'][i]\n",
        "    bid = input_recs['bid'][i]\n",
        "\n",
        "    rate = svd_model.predict(uid, bid).est\n",
        "\n",
        "    results.append([uid, bid, rate])\n",
        "\n",
        "    if not i%1000:\n",
        "      results = pd.DataFrame(results, columns = columns)\n",
        "\n",
        "      is_new_file = i==0\n",
        "      results.to_csv(temp_dir_pred,\n",
        "                     index=False, \n",
        "                     header=is_new_file, \n",
        "                     mode= \"w\" if is_new_file else \"a\"\n",
        "                     )\n",
        "      results = []\n",
        "\n",
        "    if not i%1000:\n",
        "      print(i)\n",
        "  \n",
        "  results = pd.DataFrame(results, columns = ['uid','bid','rate'])\n",
        "  results.to_csv(temp_dir_pred,\n",
        "                index=False, \n",
        "                header=False, \n",
        "                mode=\"a\"\n",
        "                )\n",
        "  \n",
        "  #here done with partial writes. re-read full file and form matrix\n",
        "  results = pd.read_csv(temp_dir_pred)\n",
        "  reverse_u_lookup = {uid:i for (i, uid) in enumerate(u_lookup)}\n",
        "  reverse_b_lookup = {bid:i for (i, bid) in enumerate(b_lookup)}\n",
        "  \n",
        "\n",
        "  rows = [ reverse_u_lookup[uid] for uid in results['uid'] ] \n",
        "  cols = [ reverse_b_lookup[bid] for bid in results['bid'] ] \n",
        "  values = results['rate']\n",
        "\n",
        "  num_u = len(reverse_u_lookup.keys())\n",
        "  num_b = len(reverse_b_lookup.keys())\n",
        "\n",
        "  sparse_pred = sparse.csr_matrix(  (values , (rows, cols)) , shape=( num_u,num_b) )\n",
        "\n",
        "  results = None\n",
        "  return sparse_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NZQtQpkAF-5"
      },
      "source": [
        "with open(os.path.join(data,\"unique_b.txt\") , 'r', encoding='utf-8') as bfile:\n",
        "  unique_b = bfile.readlines()\n",
        "  b_lookup = [b.strip() for b in unique_b]\n",
        "\n",
        "with open(os.path.join(data,\"split\",\"unique_u_test.txt\") , 'r', encoding='utf-8') as ufile:\n",
        "  unique_u = ufile.readlines()\n",
        "  test_u_lookup = [u.strip() for u in unique_u]\n",
        "\n",
        "test_test_recs = pd.read_csv(os.path.join(data,\"split/test_testing_rec.csv\")) # predict only on test testing data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmw0KoBxBv71"
      },
      "source": [
        "svd_pred = predict_with_svd(model, test_test_recs, test_u_lookup, b_lookup, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLJxU1TjF6OU"
      },
      "source": [
        "pickle.dump(svd_pred, open(os.path.join(eval_results,\"svdpp_preds.pkl\",) , \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk4jwpAlMyFg"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRafnyOFDBCd"
      },
      "source": [
        "model = pickle.load(open(os.path.join(training_results,\"svdpp_model.pkl\",) , \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uOeyrPoN0Ie"
      },
      "source": [
        "from tensorflow.keras.utils import Sequence \n",
        "from tensorflow.keras import Model, layers\n",
        "!pip3 install tensorflow-ranking\n",
        "import tensorflow_ranking as tfr\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSCrbV7kM1Qk"
      },
      "source": [
        "class datagen(Sequence):\n",
        "  def __init__(self, x_set, y_set, batch_size=500, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.y = y_set\n",
        "    self.batch_size = batch_size\n",
        "    self.max_samples_per_epoch = max_samples_per_epoch\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "    if self.max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:self.max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def __len__(self):\n",
        "      return math.ceil((self.shuffled_idx.shape[0]) / self.batch_size)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "    batch_y = np.array(self.y[idx].todense())\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Do99djNhD5"
      },
      "source": [
        "#test_true = pickle.load(open(os.path.join(data,\"split/matrices/implicit\",\"test_impl.pkl\"), \"rb\"))\n",
        "\n",
        "test_test = pickle.load(open(os.path.join(data,\"split/matrices/implicit\",\"test_testing_impl.pkl\"), \"rb\"))\n",
        "\n",
        "\n",
        "test_pred = pickle.load(open(os.path.join(eval_results,\"svdpp_preds.pkl\",) , \"rb\"))\n",
        "\n",
        "eval_datagen = datagen(x_set=test_pred ,y_set=test_test)  #evaluate against all test data, both test-training and test-testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MveZh3a7WWbk",
        "outputId": "16a2b628-3b2d-4fcf-b78a-b82ce61b4f7a"
      },
      "source": [
        "eval_datagen.__getitem__(0)[0].shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127351"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8gGO5FXV4NG"
      },
      "source": [
        "input = layers.Input(eval_datagen.__getitem__(0)[0].shape[1], name = 'input')\n",
        "\n",
        "placeholder = Model(inputs = input, outputs = input, name = 'dummy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyC3mcPAOOuL"
      },
      "source": [
        "metrics = [ tfr.keras.metrics.RecallMetric(name = \"recall_20\", topn=20,),\n",
        "           tfr.keras.metrics.RecallMetric(name = \"recall_50\", topn=50,),\n",
        "           tfr.keras.metrics.NDCGMetric(name = \"ndcg_100\", topn=100,)         \n",
        "]\n",
        "\n",
        "placeholder.compile(metrics = metrics)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7as-F4vaj-V",
        "outputId": "036a5041-ec26-471f-a9d4-565141169e0c"
      },
      "source": [
        "evals = placeholder.evaluate(eval_datagen, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 549s 27s/step - loss: 0.0000e+00 - recall_20: 0.0116 - recall_50: 0.0120 - ndcg_100: 0.0178\n"
          ]
        }
      ]
    }
  ]
}