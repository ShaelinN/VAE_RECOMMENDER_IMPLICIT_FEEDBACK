{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_V2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbFjVqXpgQ-E"
      },
      "source": [
        "#Imports and Dirs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_rNgvglTWCN"
      },
      "source": [
        "#!pip3 install tensorflow-ranking\n",
        "#!pip3 install tensorflow-probability\n",
        "#!pip3 install tensorflow\n",
        "#!pip3 install numpy\n",
        "#!pip3 install pickle\n",
        "#!pip3 install math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as tfback\n",
        "from tensorflow.keras import layers, activations, Model, losses\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint\n",
        "#import tensorflow_ranking as tfr\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2pRl4vBS7LM"
      },
      "source": [
        "#root = \"/content/drive/MyDrive/COMP700_Honours Project\"\n",
        "#inp = \"Data/remove_low_interaction_users/matrices/implicit\"\n",
        "#outp =\"models\"\n",
        "#wei = None#\"vae_epoch_01_loss_363.87.hdf5\"\n",
        "#intermediate_dim = 512 \n",
        "#total_num_epochs = 10\n",
        "#batch_size = 200\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtzkG8xR_zO"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--root', type=str)\n",
        "parser.add_argument('--input', type=str)\n",
        "parser.add_argument('--output', type=str)\n",
        "parser.add_argument('--weights', type=str)\n",
        "parser.add_argument('--intermediatedim', type=int)\n",
        "parser.add_argument('--epochs', type=int)\n",
        "parser.add_argument('--batchsize', type=int)\n",
        "parser.add_argument('--klannealrate', type=float)\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "root = args.root\n",
        "\n",
        "input_dir = args.input\n",
        "input_dir = os.path.join(root, input_dir)\n",
        "\n",
        "output_dir = args.output\n",
        "output_dir = os.path.join(root, output_dir)\n",
        "try:\n",
        "  os.mkdir(output_dir)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "#IF NO WEIGHTS FILE TO USE, THEN JUST LEAVE ARG OUT. THE LAOD FUNCTION WILL KNOW TO START ANEW\n",
        "# Checkpoint callback saves into filepath=os.path.join(output_dir,\"vae_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\")\n",
        "weights = args.weights\n",
        "try:\n",
        "  weights = os.path.join(output_dir, weights)\n",
        "except TypeError:\n",
        "  pass\n",
        "\n",
        "#annealing_rate = args.klannealrate if not args.klannealrate is None else 0.0001\n",
        "#intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "#total_num_epochs = args.epochs if args.epochs is not None else 5\n",
        "#batch_size = args.batchsize if args.batchsize is not None else 400\n",
        "\n",
        "\n",
        "\n",
        "print(root)\n",
        "print(input_dir)\n",
        "print(output_dir)\n",
        "print(weights)\n",
        "\"\"\"\n",
        "print(annealing_rate)\n",
        "print(intermediate_dim)\n",
        "print(total_num_epochs)\n",
        "print(batch_size)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8A_Q0_gVfR"
      },
      "source": [
        "#Model design\n",
        "* sample layer\n",
        "* batch history callback\n",
        "* vae loss function\n",
        "* VAE builder class\n",
        "* data generator sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5rKmGkUo_V"
      },
      "source": [
        "#distribution = tfp.distributions.LogNormal(0,1)\n",
        "#distribution = tfp.distributions.Multinomial()\n",
        "distribution = tfp.distributions.Normal(0,1)\n",
        "class Sampling(layers.Layer):\n",
        "  def __init__(self, name=\"Sampling\", **kwargs):\n",
        "    super(Sampling, self).__init__(name=name, **kwargs)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "\n",
        "\n",
        "    epsilon = distribution.sample()\n",
        "    \n",
        "    #epsilon = tfback.random_normal(shape=(batch,dim))\n",
        "    sample = epsilon * tf.exp(0.5 * z_log_var)  +   z_mean  #Reparametrization trick: convert from standard normal to desired distribution\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqvcmpfPd3Sg"
      },
      "source": [
        "class BatchHistory(keras.callbacks.Callback):  \n",
        "  def __init__(self):\n",
        "    super(BatchHistory,self).__init__() \n",
        "    self.loss = [] \n",
        "    self.val_loss = 0.0\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):   \n",
        "    #print(logs)\n",
        "    logs['recall@20'] = []\n",
        "    logs['recall@50'] = []\n",
        "    logs['ndcg'] = []\n",
        "    self.loss.append(logs.get('loss'))\n",
        "\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    self.val_loss = logs.get('val_loss')\n",
        "    self.loss = [str(i) for i in self.loss]\n",
        "\n",
        "    with open(os.path.join(output_dir,\"per_epoch_val_loss.txt\"), \"a\") as pe_v_loss:\n",
        "      pe_v_loss.write('\\n'+ str(self.val_loss))\n",
        "\n",
        "    with open(os.path.join(output_dir,\"per_batch_loss.txt\"), \"a\") as pb_loss:\n",
        "      pb_loss.write(\"\\n\")\n",
        "      pb_loss.writelines('\\n'.join(self.loss))\n",
        "\n",
        "    with open(os.path.join(output_dir,\"per_epoch_loss.txt\"), \"a\") as pe_loss:\n",
        "      pe_loss.write('\\n'+ str(self.loss[len(self.loss)-1]))\n",
        "    self.loss = []\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9mHDOrkYFlK"
      },
      "source": [
        "#original_dim = 127350\n",
        "annealing_rate = args.klannealrate if not args.klannealrate is None else 0.0001\n",
        "KLBeta = 0\n",
        "def VAE_loss(y_true, y_pred):\n",
        "    global KLBeta\n",
        "    reconst_loss =  original_dim * losses.binary_crossentropy(y_true, y_pred)\n",
        "    KLDiv = KLBeta * losses.kl_divergence(y_true, y_pred)\n",
        "\n",
        "    KLBeta = min(KLBeta+annealing_rate , 1) #update weight of KL factor\n",
        "\n",
        "    #return (KLDiv,reconst_loss)\n",
        "\n",
        "    return  reconst_loss + KLDiv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_ca8PXgjYB"
      },
      "source": [
        "class vae_builder(object):\n",
        "  def __init__(self, original_dim, intermediate_dim, latent_dim, name='VAE'):\n",
        "    self.name = name\n",
        "    self.original_dim = original_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  \n",
        "  def build(self):\n",
        "    self.input = layers.Input(self.original_dim, name = 'input')\n",
        "    self.dropout = layers.Dropout(rate=0.5)(self.input)\n",
        "\n",
        "    #encoder\n",
        "    self.e1 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e1')(self.dropout)\n",
        "    self.e2 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e2')(self.e1)\n",
        "    self.e3 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e3')(self.e2)\n",
        "    self.e4 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e4')(self.e3)\n",
        "    self.e5 = layers.Dense(self.intermediate_dim, activation=activations.hard_sigmoid, name = 'e5')(self.e4)\n",
        "\n",
        "    #sampling\n",
        "    self.mean = layers.Dense(self.latent_dim, name = 'mean')(self.e5)  \n",
        "    self.log_var = layers.Dense(self.latent_dim, name = 'log_var')(self.e5)  \n",
        "    self.sampling = Sampling()([self.mean, self.log_var])\n",
        "\n",
        "    #decoder\n",
        "    self.d1 = layers.Dense(self.intermediate_dim, activation='relu',name = 'd1')(self.sampling)\n",
        "    self.d2 = layers.Dense(self.original_dim, activation='relu', name = 'd2')(self.d1)  \n",
        "    \n",
        "    self.output = layers.Activation(activations.swish, name = 'output')(self.d2)\n",
        "\n",
        "    #to model\n",
        "    vae = Model(inputs = self.input, outputs = self.output, name = self.name)\n",
        "    return vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDjfLEDZeTr2"
      },
      "source": [
        "class datagen(keras.utils.Sequence):\n",
        "  def __init__(self, x_set, batch_size, max_samples_per_epoch=None):\n",
        "    self.x = x_set\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "    if max_samples_per_epoch is not None:\n",
        "      self.shuffled_idx = self.shuffled_idx[:max_samples_per_epoch] #cutoff at max no of samples allowed in epoch\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    self.shuffled_idx = np.arange(np.shape(self.x)[0]) #get indexes\n",
        "    np.random.shuffle(self.shuffled_idx) # shuffle\n",
        "\n",
        "  def __len__(self):\n",
        "      #return math.ceil((self.x.shape[0]) / self.batch_size)\n",
        "      return len(self.shuffled_idx)\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "    b_idx = idx * self.batch_size\n",
        "    e_idx = (idx + 1) * self.batch_size\n",
        "    idx = self.shuffled_idx[b_idx:e_idx] #cut slice of indexes using begin and end indexes\n",
        "\n",
        "    batch_x = np.array(self.x[idx].todense())\n",
        "\n",
        "    batch_y = np.array(self.x[idx].todense())\n",
        "\n",
        "\n",
        "    return batch_x , batch_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNmS-DzMgZRC"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZbIuw34Dxx"
      },
      "source": [
        "##load data and set params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnMzOLXXrXLN"
      },
      "source": [
        "train = pickle.load( \n",
        "    open(os.path.join(input_dir,\"impl_train.pkl\") , 'rb')\n",
        ")\n",
        "vad = pickle.load( \n",
        "    open(os.path.join(input_dir,\"impl_vad.pkl\") , 'rb')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aYySDmfRAF"
      },
      "source": [
        "total_num_samples = train.shape[0]\n",
        "original_dim = train.shape[1]\n",
        "intermediate_dim = args.intermediatedim if args.intermediatedim is not None else 512\n",
        "latent_dim = intermediate_dim//2 #64\n",
        "\n",
        "total_num_epochs = args.epochs if args.epochs is not None else 5\n",
        "\n",
        "batch_size = args.batchsize if args.batchsize is not None else 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ia7Da3rY8a"
      },
      "source": [
        "train_gen = datagen(train , batch_size)\n",
        "vad_gen = datagen(vad ,  batch_size, int(0.1*total_num_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5-AG8m04TBI"
      },
      "source": [
        "##code to allow continuation of training over many sessions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mlp-VtwfeyE"
      },
      "source": [
        "def load_model(v, weights_file):\n",
        "  try:\n",
        "    v.load_weights(weights_file)\n",
        "    print(\"weights loaded successfully\")\n",
        "  except:\n",
        "    print(\"failed to load weights\")\n",
        "\n",
        "def get_num_epochs_complete():\n",
        "  try:\n",
        "    with open(os.path.join(output_dir,\"per_epoch_val_loss.txt\"), \"r\") as f:\n",
        "      lines = [i for i in f.readlines() if len(i.strip()) > 0]\n",
        "      print(lines)\n",
        "      return len(lines)\n",
        "  except FileNotFoundError:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzvqmDzw4g5_"
      },
      "source": [
        "##Train code block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ugK83oxrQkA"
      },
      "source": [
        "#REFRESH CALLBACKS BEFORE EACH RUN\n",
        "#or else separate runs in same session will behave oddly\n",
        "checkpoint = ModelCheckpoint(filepath=os.path.join(output_dir,\"vae_epoch_{epoch:02d}_loss_{loss:.2f}.hdf5\"), verbose=1, save_best_only=False)\n",
        "batch_hist = BatchHistory()\n",
        "\n",
        "#BUILD STRUCTURE\n",
        "v = vae_builder(original_dim, intermediate_dim, latent_dim).build()\n",
        "\n",
        "#configurable optimiser\n",
        "slow_adam = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "\n",
        "v.compile(optimizer=slow_adam, loss=VAE_loss)\n",
        "\n",
        "#PICK UP FROM WHERE LEFT OFF\n",
        "remaining_epochs = total_num_epochs - get_num_epochs_complete()\n",
        "load_model(v, weights)\n",
        "\n",
        "#RUN\n",
        "print(\"will run for\", remaining_epochs, \"epochs unless stopped early\")\n",
        "v.fit(train_gen, verbose=True, epochs=remaining_epochs, validation_data=vad_gen, callbacks=[checkpoint,batch_hist])\n",
        "print(\"complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWE-osS1z7IT"
      },
      "source": [
        "#keras.utils.plot_model(v, show_shapes=True, to_file=os.path.join(root, \"vae.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}